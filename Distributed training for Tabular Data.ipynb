{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2c16de-0097-4d84-8b6b-f9bcddba9f76",
   "metadata": {},
   "source": [
    "## Distributed training with Built-in SageMaker Algorithms for Tabular Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabdda1-21b0-48c7-82ec-eee394265abd",
   "metadata": {},
   "source": [
    "### Setup\n",
    "This notebook was created and tested on an ml.c5.2xlarge notebook instance.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "1. The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "2. The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the get_execution_role() call with the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a722ef7-7957-4c77-a3dc-02ce288c7363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67446484-7a3a-4319-9dc1-f3383bf743e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet torch pytorch_transformers pymysql sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10363a23-2a28-418f-82d7-ab8af87413ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install heatmapz --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e32b19-cd34-4c2c-b74b-3bd13edff3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e222de74-20b8-4f63-9b63-5e94e06a26f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import json\n",
    "import joblib\n",
    "from pandas import DataFrame\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pytorch_transformers import BertModel, BertConfig, BertTokenizer\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from numpy import float32\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "87f1a29c-a618-4984-843b-5ec84b3ce017",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role = arn:aws:iam::249517808360:role/SD-workshop-0614-ExecutionRole-24GQUTL7652N\n",
      "Region = us-east-1\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.session.Session().region_name\n",
    "bucket = sess.default_bucket()\n",
    "sm = boto3.Session().client('sagemaker')\n",
    "\n",
    "print(\"Role = {}\".format(role))\n",
    "print(\"Region = {}\".format(region))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b27ba8-ffa1-42f2-881e-412e0efd8cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10a2407e-883d-4e61-8004-684c2ecfa4fd",
   "metadata": {},
   "source": [
    "### Data explore and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f013be4e-ebe8-4b4b-a56d-a86bdcd97ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da81f1e1-3676-46f6-b966-d3c779c5aa64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>3253.0</th>\n",
       "      <th>65.06</th>\n",
       "      <th>3368.9764</th>\n",
       "      <th>287.4705882352941</th>\n",
       "      <th>7073.739331026527</th>\n",
       "      <th>51.0</th>\n",
       "      <th>616.0</th>\n",
       "      <th>0.27450980392156865</th>\n",
       "      <th>...</th>\n",
       "      <th>6.061688311688313</th>\n",
       "      <th>6.5</th>\n",
       "      <th>1.0.43</th>\n",
       "      <th>376.75</th>\n",
       "      <th>30.25</th>\n",
       "      <th>1.0.44</th>\n",
       "      <th>0.75.3</th>\n",
       "      <th>0.25.7</th>\n",
       "      <th>0.375.4</th>\n",
       "      <th>3.25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4181.0</td>\n",
       "      <td>73.350877</td>\n",
       "      <td>4366.333026</td>\n",
       "      <td>306.758621</td>\n",
       "      <td>12454.424495</td>\n",
       "      <td>58.0</td>\n",
       "      <td>752.0</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>...</td>\n",
       "      <td>8.056061</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>374.600000</td>\n",
       "      <td>29.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2261.0</td>\n",
       "      <td>72.935484</td>\n",
       "      <td>9622.124870</td>\n",
       "      <td>327.312500</td>\n",
       "      <td>10273.652344</td>\n",
       "      <td>32.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>7.725397</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>31.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.582014</td>\n",
       "      <td>0.221869</td>\n",
       "      <td>0.316387</td>\n",
       "      <td>4.476886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>735.0</td>\n",
       "      <td>45.937500</td>\n",
       "      <td>2199.183594</td>\n",
       "      <td>244.823529</td>\n",
       "      <td>10196.968858</td>\n",
       "      <td>17.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>...</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.057129</td>\n",
       "      <td>385.845778</td>\n",
       "      <td>32.189208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582014</td>\n",
       "      <td>0.221869</td>\n",
       "      <td>0.316387</td>\n",
       "      <td>4.476886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2229.0</td>\n",
       "      <td>69.656250</td>\n",
       "      <td>8017.725586</td>\n",
       "      <td>247.939394</td>\n",
       "      <td>15765.693297</td>\n",
       "      <td>33.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>6.109524</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>359.200000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 1193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1.0  0.0  3253.0      65.06    3368.9764  287.4705882352941  \\\n",
       "0  1.0  0.0  4181.0  73.350877  4366.333026         306.758621   \n",
       "1  1.0  0.0  2261.0  72.935484  9622.124870         327.312500   \n",
       "2  1.0  0.0   735.0  45.937500  2199.183594         244.823529   \n",
       "3  1.0  0.0  2229.0  69.656250  8017.725586         247.939394   \n",
       "\n",
       "   7073.739331026527  51.0  616.0  0.27450980392156865  ...  \\\n",
       "0       12454.424495  58.0  752.0             0.396552  ...   \n",
       "1       10273.652344  32.0  291.0             0.062500  ...   \n",
       "2       10196.968858  17.0  275.0             0.117647  ...   \n",
       "3       15765.693297  33.0  359.0             0.272727  ...   \n",
       "\n",
       "   6.061688311688313  6.5    1.0.43      376.75      30.25  1.0.44    0.75.3  \\\n",
       "0           8.056061  8.4  1.000000  374.600000  29.800000     1.0  1.600000   \n",
       "1           7.725397  7.6  1.000000  375.000000  31.800000     1.0  0.582014   \n",
       "2           4.633333  4.0  1.057129  385.845778  32.189208     0.0  0.582014   \n",
       "3           6.109524  5.8  1.000000  359.200000  30.000000     1.0  0.000000   \n",
       "\n",
       "     0.25.7   0.375.4      3.25  \n",
       "0  0.000000  0.800000  7.000000  \n",
       "1  0.221869  0.316387  4.476886  \n",
       "2  0.221869  0.316387  4.476886  \n",
       "3  0.200000  0.000000  0.000000  \n",
       "\n",
       "[4 rows x 1193 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccce95dd-8e43-4ac3-82b2-09d42cbb4f88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18997, 1193)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3cc55a9-58ef-4fd5-9270-514a707f1504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0          float64\n",
       "0.0          float64\n",
       "3253.0       float64\n",
       "65.06        float64\n",
       "3368.9764    float64\n",
       "              ...   \n",
       "1.0.44       float64\n",
       "0.75.3       float64\n",
       "0.25.7       float64\n",
       "0.375.4      float64\n",
       "3.25         float64\n",
       "Length: 1193, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e9edb-e446-45db-9963-091d5538efd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55901efe-2e61-4354-ad4a-3cd149070e42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (6, 4)) #新建画布\n",
    "plt.hist(df['1.0'],bins = [0, 0.1,0.2, 0.5, 0.6,0.7,0.8,0.9,1],color='brown', alpha = 0.8, label = \"直方图\" ) #绘制直方图\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8a35b78-bc01-4545-8d40-d1e052883d72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9541506553666368"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['1.0']>=0.98].shape[0]/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "125e75e2-d021-46c2-9efa-520e889702b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 percentile value is 1.0\n",
      "92 percentile value is 1.0\n",
      "93 percentile value is 1.0\n",
      "94 percentile value is 1.0\n",
      "95 percentile value is 1.0\n",
      "96 percentile value is 1.0\n",
      "97 percentile value is 1.0\n",
      "98 percentile value is 1.0\n",
      "99 percentile value is 1.0\n",
      "100 percentile value is 1.0\n"
     ]
    }
   ],
   "source": [
    "# percentile capping\n",
    "qValues = np.round(np.arange(0.91, 1.01, 0.01), 3)\n",
    "quantile = df['1.0'].quantile(np.round(np.arange(0.00, 1.01, 0.01), 2))\n",
    "for i in qValues:\n",
    "    print(\"{} percentile value is {}\".format(int(i*100), quantile[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e311a25-78e3-4575-892a-6743602d3a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "'''\n",
    "for column in df.select_dtypes(include=[\"object\",'category']).columns:\n",
    "    display(pd.crosstab(index=df[column], columns=\"% observations\", normalize=\"columns\"))\n",
    "'''\n",
    "\n",
    "# Histograms for each numeric features\n",
    "#display(df.describe())\n",
    "#%matplotlib inline\n",
    "#hist = df.hist(bins=30, sharey=True, figsize=(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11dcdd82-2da7-43bf-ab14-ae425b330c79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0          7\n",
      "0.0          7\n",
      "3253.0       7\n",
      "65.06        7\n",
      "3368.9764    7\n",
      "            ..\n",
      "1.0.44       8\n",
      "0.75.3       8\n",
      "0.25.7       8\n",
      "0.375.4      8\n",
      "3.25         8\n",
      "Length: 1193, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19448651-e452-4e1a-a820-7fdc78b7d20d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "070c5ace-ecdd-42b1-80b5-22f7aaf84c58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0          0\n",
      "0.0          0\n",
      "3253.0       0\n",
      "65.06        0\n",
      "3368.9764    0\n",
      "            ..\n",
      "1.0.44       0\n",
      "0.75.3       0\n",
      "0.25.7       0\n",
      "0.375.4      0\n",
      "3.25         0\n",
      "Length: 1193, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b367275a-a7e7-4d05-8a88-db09a083f47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_parquet('df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d0643-bca5-422b-ae51-3e5b39f3d1ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_p = pd.read_parquet('df.parquet')\n",
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7eaf5bd5-6555-42b5-b814-f3e319262500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0          0\n",
      "0.0          0\n",
      "3253.0       0\n",
      "65.06        0\n",
      "3368.9764    0\n",
      "            ..\n",
      "1.0.44       0\n",
      "0.75.3       0\n",
      "0.25.7       0\n",
      "0.375.4      0\n",
      "3.25         0\n",
      "Length: 1193, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_p.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46202c75-4cc9-4887-9ca0-f2012de7756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data to train and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_set.drop('order', axis=1).fillna(0)\n",
    "y = data_set['order'].fillna(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae5817-b3ef-49a9-b6f5-c47495a28219",
   "metadata": {
    "tags": []
   },
   "source": [
    "Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7c5033c0-ad2e-41c6-bd27-ebd40f7ddcd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nboto3.Session().resource(\"s3\").Bucket(bucket).Object(\\n    os.path.join(prefix, \"validation/data.csv\")\\n).upload_file(\"validation.csv\")\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload train data to your own bucket\n",
    "prefix = \"define_by_you\"\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"train/df.parquet\")\n",
    ").upload_file(\"df.parquet\")\n",
    "'''\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"validation/data.csv\")\n",
    ").upload_file(\"validation.csv\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3aa3ede8-ddcb-4e24-9a90-2f3023b88d04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data location:s3://sagemaker-us-east-1-249517808360/7k7k/train/df.parquet\n"
     ]
    }
   ],
   "source": [
    "prefix = \"7k7k\"\n",
    "training_data_uri = 's3://{}/{}/train/df.parquet'.format(bucket,prefix)\n",
    "#test_data_uri = 's3://{}/{}/validation/data.csv'.format(bucket,prefix)\n",
    "print(\"train data location:\"+training_data_uri)\n",
    "#print(\"test data location:\"+test_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9292f-0df2-400f-bf64-33912f682e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71497971-784c-4f8e-a663-04d84f8aacd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4ab278e-31f8-40f0-949f-d3312d68f0df",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "1. distributed training with cpu\n",
    "\n",
    "| Model | Problem Type | Model ID|\n",
    "| :-- | :-- | :-- |\n",
    "| LightGBM | Classification | lightgbm-classification-model |\n",
    "| LightGBM |Regression | lightgbm-regression-model |\n",
    "| CatBoost | Classification | catboost-classification-model |\n",
    "| CatBoost | Regression | catboost-regression-model |\n",
    "| AutoGluon-Tabular | Classification | autogluon-classification-ensemble |\n",
    "| AutoGluon-Tabular |Regression | autogluon-regression-ensemble |\n",
    "| TabTransformer | Classification | pytorch-tabtransformerclassification-model |\n",
    "| TabTransformer | Regression | pytorch-tabtransformerregression-model |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d970dbd1-5f22-4b23-a675-c0e443c50e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "\n",
    "train_model_id, train_model_version, train_scope = \"lightgbm-classification-model\", \"*\", \"training\"\n",
    "training_instance_type = \"ml.m5.2xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3e64edb2-eee7-4e8c-83dd-2653cf9d327e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=train_model_id,\n",
    "    model_version=train_model_version,\n",
    "    image_scope=train_scope,\n",
    "    instance_type=training_instance_type\n",
    ")\n",
    "\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, script_scope=train_scope\n",
    ")\n",
    "\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=train_model_id, model_version=train_model_version, model_scope=train_scope\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "62ec58ea-979c-4e48-8c23-0defd55814a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample training data is available in this bucket\n",
    "training_dataset_s3_path = training_data_uri\n",
    "# validation_dataset_s3_path = f\"s3://{training_data_bucket}/{training_data_prefix}/validation\"\n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"jumpstart-example-tabular-training\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ceac9123-75ec-4e2c-94c5-97f5ce24a526",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ns3_input_validation = TrainingInput(\\n    test_data_uri, distribution= \"FullyReplicated\", content_type=\"application/x-parquet\"\\n)\\n'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "s3_input_train = TrainingInput(\n",
    "    training_data_uri, distribution= \"FullyReplicated\", content_type=\"application/x-parquet\"\n",
    ")\n",
    "'''\n",
    "s3_input_validation = TrainingInput(\n",
    "    test_data_uri, distribution= \"FullyReplicated\", content_type=\"application/x-parquet\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a24e9f84-05fc-4e38-80b4-15b9479433f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyperparameters for training the model\n",
    "hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=train_model_id, model_version=train_model_version\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ddafe3-a216-42b7-b164-3affd5ce116c",
   "metadata": {},
   "source": [
    "#### 分布式训练相关参数参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a88c96e4-c753-40a9-8376-074e90186eac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_boost_round': '500', 'metric': 'auto', 'learning_rate': '0.009', 'num_leaves': '67', 'feature_fraction': '0.74', 'bagging_fraction': '0.53', 'bagging_freq': '5', 'max_depth': '11', 'min_data_in_leaf': '26', 'max_delta_step': '0.0', 'lambda_l1': '0.0', 'lambda_l2': '0.0', 'boosting': 'gbdt', 'min_gain_to_split': '0.0', 'scale_pos_weight': '1.0', 'tree_learner': 'data', 'feature_fraction_bynode': '1.0', 'is_unbalance': 'False', 'max_bin': '255', 'num_threads': '0', 'verbosity': '1', 'use_dask': 'False'}\n"
     ]
    }
   ],
   "source": [
    "# [Optional] Override default hyperparameters with custom values\n",
    "\n",
    "hyperparameters[\"num_boost_round\"] = \"500\"\n",
    "hyperparameters[\"metric\"] = \"auc\"\n",
    "hyperparameters[\"tree_learner\"] = \"voting\"  # use AllReduce method for distributed training\n",
    "#hyperparameters[\"tree_learner\"] = \"data\"\n",
    "\n",
    "del hyperparameters[\n",
    "    \"early_stopping_rounds\"\n",
    "]  \n",
    "# current distributed training with early stopping has some issues. \n",
    "# See https://github.com/microsoft/SynapseML/issues/728#issuecomment-1221599961\n",
    "# thus it is disabled for distributed training.\n",
    "\n",
    "\n",
    "print(hyperparameters)\n",
    "\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "95844b14-f4c0-4088-b7fe-b54df68ee089",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: built-in-algo-lightgbm-classification-m-2023-11-13-13-58-21-881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-13 13:58:22 Starting - Starting the training job...\n",
      "2023-11-13 13:58:38 Starting - Preparing the instances for training......\n",
      "2023-11-13 13:59:39 Downloading - Downloading input data...\n",
      "2023-11-13 14:00:04 Training - Downloading the training image...\n",
      "2023-11-13 14:00:25 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:42,833 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:42,835 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:42,844 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:42,846 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:43,400 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/dask/dask-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/distributed/distributed-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/graphviz/graphviz-0.17-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/HeapDict/HeapDict-1.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/lightgbm/lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/locket/locket-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/msgpack/msgpack-1.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/partd/partd-1.3.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sortedcontainers/sortedcontainers-2.4.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tblib/tblib-1.7.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/toolz/toolz-0.12.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/zict/zict-2.2.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_prepack_script_utilities/sagemaker_jumpstart_prepack_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (2021.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (3.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado>=6.0.3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (1.26.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (5.6.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (0.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (0.37.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->dask==2022.12.1->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.3->-r requirements.txt (line 5)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.3->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->distributed==2022.12.1->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: toolz, locket, partd, HeapDict, zict, tblib, sortedcontainers, msgpack, dask, sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-prepack-script-utilities, lightgbm, graphviz, distributed\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:41,160 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:41,162 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:41,172 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:41,174 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:41,737 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mProcessing ./lib/dask/dask-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/distributed/distributed-2022.12.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/graphviz/graphviz-0.17-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/HeapDict/HeapDict-1.0.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/lightgbm/lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/locket/locket-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/msgpack/msgpack-1.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/partd/partd-1.3.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/sortedcontainers/sortedcontainers-2.4.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/tblib/tblib-1.7.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/toolz/toolz-0.12.0-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/zict/zict-2.2.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/sagemaker_jumpstart_tabular_script_utilities/sagemaker_jumpstart_tabular_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mProcessing ./lib/sagemaker_jumpstart_prepack_script_utilities/sagemaker_jumpstart_prepack_script_utilities-1.0.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (2021.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cloudpickle>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (2.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (21.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.8/site-packages (from dask==2022.12.1->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil>=5.0 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (5.6.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tornado>=6.0.3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (6.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (3.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from distributed==2022.12.1->-r requirements.txt (line 2)) (1.26.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (0.37.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (0.24.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (1.19.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from lightgbm==3.3.3->-r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->dask==2022.12.1->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.3->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.3->-r requirements.txt (line 5)) (2.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->distributed==2022.12.1->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[35mInstalling collected packages: toolz, locket, partd, HeapDict, zict, tblib, sortedcontainers, msgpack, dask, sagemaker-jumpstart-tabular-script-utilities, sagemaker-jumpstart-prepack-script-utilities, lightgbm, graphviz, distributed\u001b[0m\n",
      "\u001b[35mSuccessfully installed HeapDict-1.0.1 dask-2022.12.1 distributed-2022.12.1 graphviz-0.17 lightgbm-3.3.3 locket-1.0.0 msgpack-1.0.4 partd-1.3.0 sagemaker-jumpstart-prepack-script-utilities-1.0.0 sagemaker-jumpstart-tabular-script-utilities-1.0.0 sortedcontainers-2.4.0 tblib-1.7.0 toolz-0.12.0 zict-2.2.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:44,641 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:44,653 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:44,664 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:44,672 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bagging_fraction\": \"0.53\",\n",
      "        \"bagging_freq\": \"5\",\n",
      "        \"boosting\": \"gbdt\",\n",
      "        \"feature_fraction\": \"0.74\",\n",
      "        \"feature_fraction_bynode\": \"1.0\",\n",
      "        \"is_unbalance\": \"False\",\n",
      "        \"lambda_l1\": \"0.0\",\n",
      "        \"lambda_l2\": \"0.0\",\n",
      "        \"learning_rate\": \"0.009\",\n",
      "        \"max_bin\": \"255\",\n",
      "        \"max_delta_step\": \"0.0\",\n",
      "        \"max_depth\": \"11\",\n",
      "        \"metric\": \"auto\",\n",
      "        \"min_data_in_leaf\": \"26\",\n",
      "        \"min_gain_to_split\": \"0.0\",\n",
      "        \"num_boost_round\": \"500\",\n",
      "        \"num_leaves\": \"67\",\n",
      "        \"num_threads\": \"0\",\n",
      "        \"scale_pos_weight\": \"1.0\",\n",
      "        \"tree_learner\": \"data\",\n",
      "        \"use_dask\": \"False\",\n",
      "        \"verbosity\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"application/x-parquet\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"built-in-algo-lightgbm-classification-m-2023-11-13-13-58-21-881\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/lightgbm/transfer_learning/classification/prepack/v1.1.0/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auto\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"500\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"data\",\"use_dask\":\"False\",\"verbosity\":\"1\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"ContentType\":\"application/x-parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"model\",\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/lightgbm/transfer_learning/classification/prepack/v1.1.0/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auto\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"500\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"data\",\"use_dask\":\"False\",\"verbosity\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"ContentType\":\"application/x-parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"built-in-algo-lightgbm-classification-m-2023-11-13-13-58-21-881\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/lightgbm/transfer_learning/classification/prepack/v1.1.0/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--bagging_fraction\",\"0.53\",\"--bagging_freq\",\"5\",\"--boosting\",\"gbdt\",\"--feature_fraction\",\"0.74\",\"--feature_fraction_bynode\",\"1.0\",\"--is_unbalance\",\"False\",\"--lambda_l1\",\"0.0\",\"--lambda_l2\",\"0.0\",\"--learning_rate\",\"0.009\",\"--max_bin\",\"255\",\"--max_delta_step\",\"0.0\",\"--max_depth\",\"11\",\"--metric\",\"auto\",\"--min_data_in_leaf\",\"26\",\"--min_gain_to_split\",\"0.0\",\"--num_boost_round\",\"500\",\"--num_leaves\",\"67\",\"--num_threads\",\"0\",\"--scale_pos_weight\",\"1.0\",\"--tree_learner\",\"data\",\"--use_dask\",\"False\",\"--verbosity\",\"1\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_BAGGING_FRACTION=0.53\u001b[0m\n",
      "\u001b[35mSM_HP_BAGGING_FREQ=5\u001b[0m\n",
      "\u001b[35mSM_HP_BOOSTING=gbdt\u001b[0m\n",
      "\u001b[35mSM_HP_FEATURE_FRACTION=0.74\u001b[0m\n",
      "\u001b[35mSM_HP_FEATURE_FRACTION_BYNODE=1.0\u001b[0m\n",
      "\u001b[35mSM_HP_IS_UNBALANCE=False\u001b[0m\n",
      "\u001b[35mSM_HP_LAMBDA_L1=0.0\u001b[0m\n",
      "\u001b[35mSM_HP_LAMBDA_L2=0.0\u001b[0m\n",
      "\u001b[35mSM_HP_LEARNING_RATE=0.009\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_BIN=255\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_DELTA_STEP=0.0\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_DEPTH=11\u001b[0m\n",
      "\u001b[35mSM_HP_METRIC=auto\u001b[0m\n",
      "\u001b[35mSM_HP_MIN_DATA_IN_LEAF=26\u001b[0m\n",
      "\u001b[35mSM_HP_MIN_GAIN_TO_SPLIT=0.0\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_BOOST_ROUND=500\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_LEAVES=67\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_THREADS=0\u001b[0m\n",
      "\u001b[35mSM_HP_SCALE_POS_WEIGHT=1.0\u001b[0m\n",
      "\u001b[35mSM_HP_TREE_LEARNER=data\u001b[0m\n",
      "\u001b[35mSM_HP_USE_DASK=False\u001b[0m\n",
      "\u001b[35mSM_HP_VERBOSITY=1\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 transfer_learning.py --bagging_fraction 0.53 --bagging_freq 5 --boosting gbdt --feature_fraction 0.74 --feature_fraction_bynode 1.0 --is_unbalance False --lambda_l1 0.0 --lambda_l2 0.0 --learning_rate 0.009 --max_bin 255 --max_delta_step 0.0 --max_depth 11 --metric auto --min_data_in_leaf 26 --min_gain_to_split 0.0 --num_boost_round 500 --num_leaves 67 --num_threads 0 --scale_pos_weight 1.0 --tree_learner data --use_dask False --verbosity 1\u001b[0m\n",
      "\u001b[34mSuccessfully installed HeapDict-1.0.1 dask-2022.12.1 distributed-2022.12.1 graphviz-0.17 lightgbm-3.3.3 locket-1.0.0 msgpack-1.0.4 partd-1.3.0 sagemaker-jumpstart-prepack-script-utilities-1.0.0 sagemaker-jumpstart-tabular-script-utilities-1.0.0 sortedcontainers-2.4.0 tblib-1.7.0 toolz-0.12.0 zict-2.2.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:46,181 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:46,192 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:46,202 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:46,210 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"bagging_fraction\": \"0.53\",\n",
      "        \"bagging_freq\": \"5\",\n",
      "        \"boosting\": \"gbdt\",\n",
      "        \"feature_fraction\": \"0.74\",\n",
      "        \"feature_fraction_bynode\": \"1.0\",\n",
      "        \"is_unbalance\": \"False\",\n",
      "        \"lambda_l1\": \"0.0\",\n",
      "        \"lambda_l2\": \"0.0\",\n",
      "        \"learning_rate\": \"0.009\",\n",
      "        \"max_bin\": \"255\",\n",
      "        \"max_delta_step\": \"0.0\",\n",
      "        \"max_depth\": \"11\",\n",
      "        \"metric\": \"auto\",\n",
      "        \"min_data_in_leaf\": \"26\",\n",
      "        \"min_gain_to_split\": \"0.0\",\n",
      "        \"num_boost_round\": \"500\",\n",
      "        \"num_leaves\": \"67\",\n",
      "        \"num_threads\": \"0\",\n",
      "        \"scale_pos_weight\": \"1.0\",\n",
      "        \"tree_learner\": \"data\",\n",
      "        \"use_dask\": \"False\",\n",
      "        \"verbosity\": \"1\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"application/x-parquet\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"built-in-algo-lightgbm-classification-m-2023-11-13-13-58-21-881\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/lightgbm/transfer_learning/classification/prepack/v1.1.0/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auto\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"500\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"data\",\"use_dask\":\"False\",\"verbosity\":\"1\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"ContentType\":\"application/x-parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/lightgbm/transfer_learning/classification/prepack/v1.1.0/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"bagging_fraction\":\"0.53\",\"bagging_freq\":\"5\",\"boosting\":\"gbdt\",\"feature_fraction\":\"0.74\",\"feature_fraction_bynode\":\"1.0\",\"is_unbalance\":\"False\",\"lambda_l1\":\"0.0\",\"lambda_l2\":\"0.0\",\"learning_rate\":\"0.009\",\"max_bin\":\"255\",\"max_delta_step\":\"0.0\",\"max_depth\":\"11\",\"metric\":\"auto\",\"min_data_in_leaf\":\"26\",\"min_gain_to_split\":\"0.0\",\"num_boost_round\":\"500\",\"num_leaves\":\"67\",\"num_threads\":\"0\",\"scale_pos_weight\":\"1.0\",\"tree_learner\":\"data\",\"use_dask\":\"False\",\"verbosity\":\"1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"ContentType\":\"application/x-parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"built-in-algo-lightgbm-classification-m-2023-11-13-13-58-21-881\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/lightgbm/transfer_learning/classification/prepack/v1.1.0/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bagging_fraction\",\"0.53\",\"--bagging_freq\",\"5\",\"--boosting\",\"gbdt\",\"--feature_fraction\",\"0.74\",\"--feature_fraction_bynode\",\"1.0\",\"--is_unbalance\",\"False\",\"--lambda_l1\",\"0.0\",\"--lambda_l2\",\"0.0\",\"--learning_rate\",\"0.009\",\"--max_bin\",\"255\",\"--max_delta_step\",\"0.0\",\"--max_depth\",\"11\",\"--metric\",\"auto\",\"--min_data_in_leaf\",\"26\",\"--min_gain_to_split\",\"0.0\",\"--num_boost_round\",\"500\",\"--num_leaves\",\"67\",\"--num_threads\",\"0\",\"--scale_pos_weight\",\"1.0\",\"--tree_learner\",\"data\",\"--use_dask\",\"False\",\"--verbosity\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BAGGING_FRACTION=0.53\u001b[0m\n",
      "\u001b[34mSM_HP_BAGGING_FREQ=5\u001b[0m\n",
      "\u001b[34mSM_HP_BOOSTING=gbdt\u001b[0m\n",
      "\u001b[34mSM_HP_FEATURE_FRACTION=0.74\u001b[0m\n",
      "\u001b[34mSM_HP_FEATURE_FRACTION_BYNODE=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_IS_UNBALANCE=False\u001b[0m\n",
      "\u001b[34mSM_HP_LAMBDA_L1=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_LAMBDA_L2=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.009\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_BIN=255\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DELTA_STEP=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=11\u001b[0m\n",
      "\u001b[34mSM_HP_METRIC=auto\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_DATA_IN_LEAF=26\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_GAIN_TO_SPLIT=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_BOOST_ROUND=500\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LEAVES=67\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_THREADS=0\u001b[0m\n",
      "\u001b[34mSM_HP_SCALE_POS_WEIGHT=1.0\u001b[0m\n",
      "\u001b[34mSM_HP_TREE_LEARNER=data\u001b[0m\n",
      "\u001b[34mSM_HP_USE_DASK=False\u001b[0m\n",
      "\u001b[34mSM_HP_VERBOSITY=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 transfer_learning.py --bagging_fraction 0.53 --bagging_freq 5 --boosting gbdt --feature_fraction 0.74 --feature_fraction_bynode 1.0 --is_unbalance False --lambda_l1 0.0 --lambda_l2 0.0 --learning_rate 0.009 --max_bin 255 --max_delta_step 0.0 --max_depth 11 --metric auto --min_data_in_leaf 26 --min_gain_to_split 0.0 --num_boost_round 500 --num_leaves 67 --num_threads 0 --scale_pos_weight 1.0 --tree_learner data --use_dask False --verbosity 1\u001b[0m\n",
      "\u001b[35mINFO:root:Initializing a Dask cluster\u001b[0m\n",
      "\u001b[35mINFO:root:Start Dask cluster in all nodes\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/distributed/cli/dask_worker.py:266: FutureWarning: dask-worker is deprecated and will be removed in a future release; use `dask worker` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mINFO:root:Received a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[34mINFO:root:Initializing a Dask cluster\u001b[0m\n",
      "\u001b[34mINFO:root:Start Dask cluster in all nodes\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:47,886 - distributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/distributed/cli/dask_worker.py:266: FutureWarning: dask-worker is deprecated and will be removed in a future release; use `dask worker` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/distributed/dashboard/core.py:20: UserWarning: \u001b[0m\n",
      "\u001b[34mDask needs bokeh >= 2.4.2, < 3 for the dashboard.\u001b[0m\n",
      "\u001b[34mYou have bokeh==2.4.0.\u001b[0m\n",
      "\u001b[34mContinuing without the dashboard.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:48,215 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:48,217 - distributed.scheduler - INFO - State start\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:48,219 - distributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:48,219 - distributed.scheduler - INFO -   Scheduler at:   tcp://10.0.228.115:8786\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:48,219 - distributed.scheduler - INFO -   dashboard at:                     :8787\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:48,352 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.228.115:37323'\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/distributed/dashboard/core.py:20: UserWarning: \u001b[0m\n",
      "\u001b[34mDask needs bokeh >= 2.4.2, < 3 for the dashboard.\u001b[0m\n",
      "\u001b[34mYou have bokeh==2.4.0.\u001b[0m\n",
      "\u001b[34mContinuing without the dashboard.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,099 - distributed.worker - INFO -       Start worker at:    tcp://10.0.228.115:9000\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,099 - distributed.worker - INFO -          Listening to:    tcp://10.0.228.115:9000\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,099 - distributed.worker - INFO -          dashboard at:         10.0.228.115:46103\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,099 - distributed.worker - INFO - Waiting to connect to:    tcp://10.0.228.115:8786\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,099 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,099 - distributed.worker - INFO -               Threads:                          8\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,099 - distributed.worker - INFO -                Memory:                  28.84 GiB\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,099 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q3m6kz3i\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,099 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,104 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.0.228.115:9000', status: init, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,106 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.228.115:9000\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,106 - distributed.core - INFO - Starting established connection to tcp://10.0.228.115:39886\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,106 - distributed.worker - INFO -         Registered to:    tcp://10.0.228.115:8786\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,106 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,108 - distributed.core - INFO - Starting established connection to tcp://10.0.228.115:8786\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,472 - distributed.scheduler - INFO - Receive client connection: Client-0cc7e25e-822d-11ee-8029-f6da49ceb264\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:49,472 - distributed.core - INFO - Starting established connection to tcp://10.0.228.115:39888\u001b[0m\n",
      "\u001b[34mINFO:root:Client summary: <Client: 'tcp://10.0.228.115:8786' processes=1 threads=8, memory=28.84 GiB>.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading data\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:50,067 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.219.142:41569'\u001b[0m\n",
      "\u001b[34mINFO:root:Validation data is not found. 20.0% of training data is randomly selected as validation data. The seed for random sampling is 200.\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:50,849 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.0.219.142:9000', status: init, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:50,849 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.219.142:9000\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:50,849 - distributed.core - INFO - Starting established connection to tcp://10.0.219.142:58070\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.8/site-packages/distributed/dashboard/core.py:20: UserWarning: \u001b[0m\n",
      "\u001b[35mDask needs bokeh >= 2.4.2, < 3 for the dashboard.\u001b[0m\n",
      "\u001b[35mYou have bokeh==2.4.0.\u001b[0m\n",
      "\u001b[35mContinuing without the dashboard.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:50,843 - distributed.worker - INFO -       Start worker at:    tcp://10.0.219.142:9000\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:50,843 - distributed.worker - INFO -          Listening to:    tcp://10.0.219.142:9000\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:50,843 - distributed.worker - INFO -          dashboard at:         10.0.219.142:45557\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:50,843 - distributed.worker - INFO - Waiting to connect to:    tcp://10.0.228.115:8786\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:50,843 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:50,843 - distributed.worker - INFO -               Threads:                          8\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:50,843 - distributed.worker - INFO -                Memory:                  28.83 GiB\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:50,843 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j1nj5ga7\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:50,843 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:50,849 - distributed.worker - INFO -         Registered to:    tcp://10.0.228.115:8786\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:50,850 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:50,850 - distributed.core - INFO - Starting established connection to tcp://10.0.228.115:8786\u001b[0m\n",
      "\u001b[34m2023-11-13 14:00:52,072 - distributed.utils_perf - INFO - full garbage collection released 174.50 MiB from 0 reference cycles (threshold: 9.54 MiB)\u001b[0m\n",
      "\u001b[34mINFO:root:'_input_model_extracted/__models_info__.json' file could not be found.\u001b[0m\n",
      "\u001b[34mWARNING:root:Disable early stopping feature in multi-instance dask training due to an issue in the open sourced LightGBM repository. For details, see https://github.com/microsoft/SynapseML/issues/728#issuecomment-1221599961\u001b[0m\n",
      "\u001b[34mINFO:root:Beginning training\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:525: UserWarning: Parameter num_threads will be ignored.\n",
      "  _log_warning(f\"Parameter {param_alias} will be ignored.\")\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/lightgbm/dask.py:525: UserWarning: Parameter n_jobs will be ignored.\n",
      "  _log_warning(f\"Parameter {param_alias} will be ignored.\")\u001b[0m\n",
      "\u001b[35m2023-11-13 14:00:56,570 - distributed.utils_perf - INFO - full garbage collection released 22.74 MiB from 26 reference cycles (threshold: 9.54 MiB)\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Trying to bind port 52297...\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Binding port 52297 succeeded\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Listening...\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Connected to rank 1\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info] Local rank: 0, total number of machines: 2\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[35mmin_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[35mmin_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[35mfeature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[35mboosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[35mbagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[35mlambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[35mbagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[35mnum_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[35mlambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info]\u001b[0m\n",
      "\u001b[35mNumber of positive: 14540, number of negative: 691\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[35mAuto-choosing col-wise multi-threading, the overhead of testing was 0.103000 seconds.\u001b[0m\n",
      "\u001b[35mYou can set `force_col_wise=true` to remove the overhead.\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info]\u001b[0m\n",
      "\u001b[35mTotal Bins 191807\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info]\u001b[0m\n",
      "\u001b[35mNumber of data points in the train set: 10162, number of used features: 1191\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info]\u001b[0m\n",
      "\u001b[35m[binary:BoostFromScore]: pavg=0.954632 -> initscore=3.046519\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info]\u001b[0m\n",
      "\u001b[35mStart training from score 3.046519\u001b[0m\n",
      "\u001b[34mFinding random open ports for workers\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Trying to bind port 41057...\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Binding port 41057 succeeded\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Listening...\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Connected to rank 0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Local rank: 1, total number of machines: 2\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[34mmin_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[34mmin_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[34mfeature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[34mboosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[34mbagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[34mlambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[34mbagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[34mnum_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[34mlambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Number of positive: 14540, number of negative: 691\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[34mAuto-choosing col-wise multi-threading, the overhead of testing was 0.062075 seconds.\u001b[0m\n",
      "\u001b[34mYou can set `force_col_wise=true` to remove the overhead.\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info]\u001b[0m\n",
      "\u001b[34mTotal Bins 191807\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info]\u001b[0m\n",
      "\u001b[34mNumber of data points in the train set: 5069, number of used features: 1191\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] min_gain_to_split is set=0.0, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=26\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] lambda_l2 is set=0.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] bagging_fraction is set=0.53, subsample=1.0 will be ignored. Current value: bagging_fraction=0.53\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] num_threads is set=8, n_jobs=-1 will be ignored. Current value: num_threads=8\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] lambda_l1 is set=0.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.0\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.954632 -> initscore=3.046519\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Start training from score 3.046519\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[1]#011val's binary_logloss: 0.210163#011train's binary_logloss: 0.173098\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[35mNo further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[1]#011val's binary_logloss: 0.174283#011train's binary_logloss: 0.189153\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[2]#011val's binary_logloss: 0.209726#011train's binary_logloss: 0.172338\u001b[0m\n",
      "\u001b[34m[3]#011val's binary_logloss: 0.209432#011train's binary_logloss: 0.171525\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[2]#011val's binary_logloss: 0.173903#011train's binary_logloss: 0.188269\u001b[0m\n",
      "\u001b[35m[3]#011val's binary_logloss: 0.173572#011train's binary_logloss: 0.187399\u001b[0m\n",
      "\u001b[35m[4]#011val's binary_logloss: 0.173287#011train's binary_logloss: 0.186344\u001b[0m\n",
      "\u001b[34m[4]#011val's binary_logloss: 0.208904#011train's binary_logloss: 0.170515\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[5]#011val's binary_logloss: 0.208559#011train's binary_logloss: 0.169906\u001b[0m\n",
      "\u001b[34m[6]#011val's binary_logloss: 0.208202#011train's binary_logloss: 0.169059\u001b[0m\n",
      "\u001b[34m[7]#011val's binary_logloss: 0.207802#011train's binary_logloss: 0.168346\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[5]#011val's binary_logloss: 0.172948#011train's binary_logloss: 0.185531\u001b[0m\n",
      "\u001b[35m[6]#011val's binary_logloss: 0.172544#011train's binary_logloss: 0.184581\u001b[0m\n",
      "\u001b[35m[7]#011val's binary_logloss: 0.172244#011train's binary_logloss: 0.183695\u001b[0m\n",
      "\u001b[34m[8]#011val's binary_logloss: 0.207465#011train's binary_logloss: 0.167568\u001b[0m\n",
      "\u001b[35m[8]#011val's binary_logloss: 0.171956#011train's binary_logloss: 0.182883\u001b[0m\n",
      "\u001b[35m[9]#011val's binary_logloss: 0.171719#011train's binary_logloss: 0.181983\u001b[0m\n",
      "\u001b[34m[9]#011val's binary_logloss: 0.207215#011train's binary_logloss: 0.166871\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[10]#011val's binary_logloss: 0.206938#011train's binary_logloss: 0.166121\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[11]#011val's binary_logloss: 0.206581#011train's binary_logloss: 0.165562\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[10]#011val's binary_logloss: 0.171544#011train's binary_logloss: 0.181249\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[11]#011val's binary_logloss: 0.171241#011train's binary_logloss: 0.180539\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[12]#011val's binary_logloss: 0.206311#011train's binary_logloss: 0.164935\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[13]#011val's binary_logloss: 0.206051#011train's binary_logloss: 0.164277\u001b[0m\n",
      "\u001b[34m[14]#011val's binary_logloss: 0.205787#011train's binary_logloss: 0.163504\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[15]#011val's binary_logloss: 0.205498#011train's binary_logloss: 0.162878\u001b[0m\n",
      "\u001b[34m[16]#011val's binary_logloss: 0.204941#011train's binary_logloss: 0.162149\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[12]#011val's binary_logloss: 0.170941#011train's binary_logloss: 0.179828\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[13]#011val's binary_logloss: 0.17057#011train's binary_logloss: 0.179089\u001b[0m\n",
      "\u001b[35m[14]#011val's binary_logloss: 0.170261#011train's binary_logloss: 0.178162\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[15]#011val's binary_logloss: 0.169912#011train's binary_logloss: 0.177491\u001b[0m\n",
      "\u001b[35m[16]#011val's binary_logloss: 0.169748#011train's binary_logloss: 0.176553\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[17]#011val's binary_logloss: 0.169599#011train's binary_logloss: 0.175818\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[18]#011val's binary_logloss: 0.169551#011train's binary_logloss: 0.175083\u001b[0m\n",
      "\u001b[35m[19]#011val's binary_logloss: 0.169309#011train's binary_logloss: 0.174197\u001b[0m\n",
      "\u001b[35m[20]#011val's binary_logloss: 0.169164#011train's binary_logloss: 0.173389\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[21]#011val's binary_logloss: 0.169085#011train's binary_logloss: 0.172762\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[22]#011val's binary_logloss: 0.168978#011train's binary_logloss: 0.172066\u001b[0m\n",
      "\u001b[35m[23]#011val's binary_logloss: 0.168815#011train's binary_logloss: 0.171431\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[24]#011val's binary_logloss: 0.168643#011train's binary_logloss: 0.170956\u001b[0m\n",
      "\u001b[35m[25]#011val's binary_logloss: 0.168544#011train's binary_logloss: 0.170317\u001b[0m\n",
      "\u001b[35m[26]#011val's binary_logloss: 0.168616#011train's binary_logloss: 0.169477\u001b[0m\n",
      "\u001b[35m[27]#011val's binary_logloss: 0.168472#011train's binary_logloss: 0.168655\u001b[0m\n",
      "\u001b[35m[28]#011val's binary_logloss: 0.168326#011train's binary_logloss: 0.167899\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[29]#011val's binary_logloss: 0.168185#011train's binary_logloss: 0.167211\u001b[0m\n",
      "\u001b[35m[30]#011val's binary_logloss: 0.168031#011train's binary_logloss: 0.166542\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[31]#011val's binary_logloss: 0.167938#011train's binary_logloss: 0.16592\u001b[0m\n",
      "\u001b[35m[32]#011val's binary_logloss: 0.167868#011train's binary_logloss: 0.165266\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[33]#011val's binary_logloss: 0.167664#011train's binary_logloss: 0.164558\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[34]#011val's binary_logloss: 0.167481#011train's binary_logloss: 0.163953\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[35]#011val's binary_logloss: 0.167454#011train's binary_logloss: 0.163452\u001b[0m\n",
      "\u001b[35m[36]#011val's binary_logloss: 0.167375#011train's binary_logloss: 0.162683\u001b[0m\n",
      "\u001b[35m[37]#011val's binary_logloss: 0.167089#011train's binary_logloss: 0.162009\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[38]#011val's binary_logloss: 0.166984#011train's binary_logloss: 0.161359\u001b[0m\n",
      "\u001b[35m[39]#011val's binary_logloss: 0.166934#011train's binary_logloss: 0.160628\u001b[0m\n",
      "\u001b[35m[40]#011val's binary_logloss: 0.166702#011train's binary_logloss: 0.159887\u001b[0m\n",
      "\u001b[35m[41]#011val's binary_logloss: 0.166531#011train's binary_logloss: 0.159307\u001b[0m\n",
      "\u001b[35m[42]#011val's binary_logloss: 0.166438#011train's binary_logloss: 0.158755\u001b[0m\n",
      "\u001b[35m[43]#011val's binary_logloss: 0.166351#011train's binary_logloss: 0.158153\u001b[0m\n",
      "\u001b[35m[44]#011val's binary_logloss: 0.166237#011train's binary_logloss: 0.157602\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[45]#011val's binary_logloss: 0.166145#011train's binary_logloss: 0.157076\u001b[0m\n",
      "\u001b[35m[46]#011val's binary_logloss: 0.166082#011train's binary_logloss: 0.156542\u001b[0m\n",
      "\u001b[35m[47]#011val's binary_logloss: 0.166005#011train's binary_logloss: 0.155965\u001b[0m\n",
      "\u001b[35m[48]#011val's binary_logloss: 0.165916#011train's binary_logloss: 0.155371\u001b[0m\n",
      "\u001b[35m[49]#011val's binary_logloss: 0.165807#011train's binary_logloss: 0.154699\u001b[0m\n",
      "\u001b[35m[50]#011val's binary_logloss: 0.165746#011train's binary_logloss: 0.154114\u001b[0m\n",
      "\u001b[35m[51]#011val's binary_logloss: 0.165622#011train's binary_logloss: 0.153411\u001b[0m\n",
      "\u001b[35m[52]#011val's binary_logloss: 0.165571#011train's binary_logloss: 0.152659\u001b[0m\n",
      "\u001b[35m[53]#011val's binary_logloss: 0.165485#011train's binary_logloss: 0.151991\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[54]#011val's binary_logloss: 0.165411#011train's binary_logloss: 0.151475\u001b[0m\n",
      "\u001b[35m[55]#011val's binary_logloss: 0.16538#011train's binary_logloss: 0.150924\u001b[0m\n",
      "\u001b[35m[56]#011val's binary_logloss: 0.165224#011train's binary_logloss: 0.150339\u001b[0m\n",
      "\u001b[35m[57]#011val's binary_logloss: 0.165137#011train's binary_logloss: 0.149679\u001b[0m\n",
      "\u001b[35m[58]#011val's binary_logloss: 0.165047#011train's binary_logloss: 0.149111\u001b[0m\n",
      "\u001b[35m[59]#011val's binary_logloss: 0.164974#011train's binary_logloss: 0.148548\u001b[0m\n",
      "\u001b[35m[60]#011val's binary_logloss: 0.164942#011train's binary_logloss: 0.148099\u001b[0m\n",
      "\u001b[35m[61]#011val's binary_logloss: 0.16485#011train's binary_logloss: 0.147588\u001b[0m\n",
      "\u001b[35m[62]#011val's binary_logloss: 0.164712#011train's binary_logloss: 0.147083\u001b[0m\n",
      "\u001b[35m[63]#011val's binary_logloss: 0.164646#011train's binary_logloss: 0.146506\u001b[0m\n",
      "\u001b[35m[64]#011val's binary_logloss: 0.164537#011train's binary_logloss: 0.146064\u001b[0m\n",
      "\u001b[35m[65]#011val's binary_logloss: 0.164416#011train's binary_logloss: 0.145462\u001b[0m\n",
      "\u001b[35m[66]#011val's binary_logloss: 0.16439#011train's binary_logloss: 0.144959\u001b[0m\n",
      "\u001b[35m[67]#011val's binary_logloss: 0.164315#011train's binary_logloss: 0.144532\u001b[0m\n",
      "\u001b[35m[68]#011val's binary_logloss: 0.164199#011train's binary_logloss: 0.143883\u001b[0m\n",
      "\u001b[35m[69]#011val's binary_logloss: 0.164115#011train's binary_logloss: 0.143427\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[17]#011val's binary_logloss: 0.204732#011train's binary_logloss: 0.161504\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[18]#011val's binary_logloss: 0.204319#011train's binary_logloss: 0.160874\u001b[0m\n",
      "\u001b[34m[19]#011val's binary_logloss: 0.204037#011train's binary_logloss: 0.160107\u001b[0m\n",
      "\u001b[34m[20]#011val's binary_logloss: 0.203411#011train's binary_logloss: 0.159464\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[21]#011val's binary_logloss: 0.203218#011train's binary_logloss: 0.158851\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[22]#011val's binary_logloss: 0.202716#011train's binary_logloss: 0.158153\u001b[0m\n",
      "\u001b[34m[23]#011val's binary_logloss: 0.202223#011train's binary_logloss: 0.157298\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[24]#011val's binary_logloss: 0.201679#011train's binary_logloss: 0.156726\u001b[0m\n",
      "\u001b[34m[25]#011val's binary_logloss: 0.201229#011train's binary_logloss: 0.156041\u001b[0m\n",
      "\u001b[34m[26]#011val's binary_logloss: 0.200929#011train's binary_logloss: 0.15541\u001b[0m\n",
      "\u001b[34m[27]#011val's binary_logloss: 0.200708#011train's binary_logloss: 0.154779\u001b[0m\n",
      "\u001b[34m[28]#011val's binary_logloss: 0.200439#011train's binary_logloss: 0.154072\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[29]#011val's binary_logloss: 0.20011#011train's binary_logloss: 0.153558\u001b[0m\n",
      "\u001b[34m[30]#011val's binary_logloss: 0.199734#011train's binary_logloss: 0.153062\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[31]#011val's binary_logloss: 0.199414#011train's binary_logloss: 0.152539\u001b[0m\n",
      "\u001b[34m[32]#011val's binary_logloss: 0.198868#011train's binary_logloss: 0.152014\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[33]#011val's binary_logloss: 0.19881#011train's binary_logloss: 0.151452\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[34]#011val's binary_logloss: 0.198736#011train's binary_logloss: 0.150957\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[35]#011val's binary_logloss: 0.19864#011train's binary_logloss: 0.150561\u001b[0m\n",
      "\u001b[34m[36]#011val's binary_logloss: 0.19855#011train's binary_logloss: 0.149892\u001b[0m\n",
      "\u001b[34m[37]#011val's binary_logloss: 0.198355#011train's binary_logloss: 0.149313\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[38]#011val's binary_logloss: 0.198079#011train's binary_logloss: 0.148801\u001b[0m\n",
      "\u001b[34m[39]#011val's binary_logloss: 0.197823#011train's binary_logloss: 0.14825\u001b[0m\n",
      "\u001b[34m[40]#011val's binary_logloss: 0.197482#011train's binary_logloss: 0.147614\u001b[0m\n",
      "\u001b[34m[41]#011val's binary_logloss: 0.197267#011train's binary_logloss: 0.147084\u001b[0m\n",
      "\u001b[34m[42]#011val's binary_logloss: 0.197105#011train's binary_logloss: 0.146585\u001b[0m\n",
      "\u001b[34m[43]#011val's binary_logloss: 0.196968#011train's binary_logloss: 0.146083\u001b[0m\n",
      "\u001b[34m[44]#011val's binary_logloss: 0.196688#011train's binary_logloss: 0.145683\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[45]#011val's binary_logloss: 0.196565#011train's binary_logloss: 0.145254\u001b[0m\n",
      "\u001b[34m[46]#011val's binary_logloss: 0.196355#011train's binary_logloss: 0.144729\u001b[0m\n",
      "\u001b[34m[47]#011val's binary_logloss: 0.196215#011train's binary_logloss: 0.144189\u001b[0m\n",
      "\u001b[34m[48]#011val's binary_logloss: 0.195889#011train's binary_logloss: 0.14357\u001b[0m\n",
      "\u001b[34m[49]#011val's binary_logloss: 0.195736#011train's binary_logloss: 0.143007\u001b[0m\n",
      "\u001b[34m[50]#011val's binary_logloss: 0.195637#011train's binary_logloss: 0.142443\u001b[0m\n",
      "\u001b[34m[51]#011val's binary_logloss: 0.195332#011train's binary_logloss: 0.141805\u001b[0m\n",
      "\u001b[34m[52]#011val's binary_logloss: 0.195038#011train's binary_logloss: 0.141175\u001b[0m\n",
      "\u001b[34m[53]#011val's binary_logloss: 0.194996#011train's binary_logloss: 0.140572\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[54]#011val's binary_logloss: 0.194765#011train's binary_logloss: 0.140171\u001b[0m\n",
      "\u001b[34m[55]#011val's binary_logloss: 0.194673#011train's binary_logloss: 0.13969\u001b[0m\n",
      "\u001b[34m[56]#011val's binary_logloss: 0.194701#011train's binary_logloss: 0.139106\u001b[0m\n",
      "\u001b[34m[57]#011val's binary_logloss: 0.194655#011train's binary_logloss: 0.138525\u001b[0m\n",
      "\u001b[34m[58]#011val's binary_logloss: 0.194476#011train's binary_logloss: 0.137973\u001b[0m\n",
      "\u001b[34m[59]#011val's binary_logloss: 0.194287#011train's binary_logloss: 0.137443\u001b[0m\n",
      "\u001b[34m[60]#011val's binary_logloss: 0.194284#011train's binary_logloss: 0.136922\u001b[0m\n",
      "\u001b[34m[61]#011val's binary_logloss: 0.194201#011train's binary_logloss: 0.136456\u001b[0m\n",
      "\u001b[34m[62]#011val's binary_logloss: 0.194061#011train's binary_logloss: 0.136019\u001b[0m\n",
      "\u001b[34m[63]#011val's binary_logloss: 0.193982#011train's binary_logloss: 0.135424\u001b[0m\n",
      "\u001b[34m[64]#011val's binary_logloss: 0.193956#011train's binary_logloss: 0.135007\u001b[0m\n",
      "\u001b[34m[65]#011val's binary_logloss: 0.194037#011train's binary_logloss: 0.13449\u001b[0m\n",
      "\u001b[34m[66]#011val's binary_logloss: 0.194012#011train's binary_logloss: 0.134\u001b[0m\n",
      "\u001b[34m[67]#011val's binary_logloss: 0.193853#011train's binary_logloss: 0.133539\u001b[0m\n",
      "\u001b[34m[68]#011val's binary_logloss: 0.193916#011train's binary_logloss: 0.132887\u001b[0m\n",
      "\u001b[34m[69]#011val's binary_logloss: 0.193897#011train's binary_logloss: 0.132421\u001b[0m\n",
      "\u001b[34m[70]#011val's binary_logloss: 0.193705#011train's binary_logloss: 0.131895\u001b[0m\n",
      "\u001b[34m[71]#011val's binary_logloss: 0.193448#011train's binary_logloss: 0.131572\u001b[0m\n",
      "\u001b[35m[70]#011val's binary_logloss: 0.163975#011train's binary_logloss: 0.14289\u001b[0m\n",
      "\u001b[35m[71]#011val's binary_logloss: 0.163868#011train's binary_logloss: 0.14241\u001b[0m\n",
      "\u001b[35m[72]#011val's binary_logloss: 0.16382#011train's binary_logloss: 0.141884\u001b[0m\n",
      "\u001b[34m[72]#011val's binary_logloss: 0.193404#011train's binary_logloss: 0.131071\u001b[0m\n",
      "\u001b[34m[73]#011val's binary_logloss: 0.193394#011train's binary_logloss: 0.130732\u001b[0m\n",
      "\u001b[34m[74]#011val's binary_logloss: 0.193417#011train's binary_logloss: 0.130333\u001b[0m\n",
      "\u001b[34m[75]#011val's binary_logloss: 0.193319#011train's binary_logloss: 0.129838\u001b[0m\n",
      "\u001b[35m[73]#011val's binary_logloss: 0.163756#011train's binary_logloss: 0.14145\u001b[0m\n",
      "\u001b[35m[74]#011val's binary_logloss: 0.163639#011train's binary_logloss: 0.141013\u001b[0m\n",
      "\u001b[35m[75]#011val's binary_logloss: 0.163552#011train's binary_logloss: 0.140481\u001b[0m\n",
      "\u001b[34m[76]#011val's binary_logloss: 0.193019#011train's binary_logloss: 0.129253\u001b[0m\n",
      "\u001b[34m[77]#011val's binary_logloss: 0.192861#011train's binary_logloss: 0.128718\u001b[0m\n",
      "\u001b[34m[78]#011val's binary_logloss: 0.192716#011train's binary_logloss: 0.128263\u001b[0m\n",
      "\u001b[35m[76]#011val's binary_logloss: 0.163407#011train's binary_logloss: 0.139871\u001b[0m\n",
      "\u001b[35m[77]#011val's binary_logloss: 0.163341#011train's binary_logloss: 0.139316\u001b[0m\n",
      "\u001b[35m[78]#011val's binary_logloss: 0.163166#011train's binary_logloss: 0.138875\u001b[0m\n",
      "\u001b[34m[79]#011val's binary_logloss: 0.192523#011train's binary_logloss: 0.127966\u001b[0m\n",
      "\u001b[34m[80]#011val's binary_logloss: 0.192373#011train's binary_logloss: 0.127437\u001b[0m\n",
      "\u001b[34m[81]#011val's binary_logloss: 0.19215#011train's binary_logloss: 0.126877\u001b[0m\n",
      "\u001b[35m[79]#011val's binary_logloss: 0.163107#011train's binary_logloss: 0.138568\u001b[0m\n",
      "\u001b[35m[80]#011val's binary_logloss: 0.16299#011train's binary_logloss: 0.138048\u001b[0m\n",
      "\u001b[35m[81]#011val's binary_logloss: 0.162873#011train's binary_logloss: 0.137437\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[82]#011val's binary_logloss: 0.162804#011train's binary_logloss: 0.137086\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[82]#011val's binary_logloss: 0.192095#011train's binary_logloss: 0.126507\u001b[0m\n",
      "\u001b[34m[83]#011val's binary_logloss: 0.192045#011train's binary_logloss: 0.126209\u001b[0m\n",
      "\u001b[35m[83]#011val's binary_logloss: 0.162712#011train's binary_logloss: 0.136701\u001b[0m\n",
      "\u001b[35m[84]#011val's binary_logloss: 0.162662#011train's binary_logloss: 0.136056\u001b[0m\n",
      "\u001b[34m[84]#011val's binary_logloss: 0.192016#011train's binary_logloss: 0.125654\u001b[0m\n",
      "\u001b[34m[85]#011val's binary_logloss: 0.19195#011train's binary_logloss: 0.125186\u001b[0m\n",
      "\u001b[34m[86]#011val's binary_logloss: 0.191836#011train's binary_logloss: 0.124896\u001b[0m\n",
      "\u001b[35m[85]#011val's binary_logloss: 0.162613#011train's binary_logloss: 0.135602\u001b[0m\n",
      "\u001b[35m[86]#011val's binary_logloss: 0.162552#011train's binary_logloss: 0.135039\u001b[0m\n",
      "\u001b[35m[87]#011val's binary_logloss: 0.162522#011train's binary_logloss: 0.134524\u001b[0m\n",
      "\u001b[34m[87]#011val's binary_logloss: 0.1918#011train's binary_logloss: 0.124523\u001b[0m\n",
      "\u001b[34m[88]#011val's binary_logloss: 0.191708#011train's binary_logloss: 0.124159\u001b[0m\n",
      "\u001b[34m[89]#011val's binary_logloss: 0.191492#011train's binary_logloss: 0.123701\u001b[0m\n",
      "\u001b[35m[88]#011val's binary_logloss: 0.162485#011train's binary_logloss: 0.133983\u001b[0m\n",
      "\u001b[35m[89]#011val's binary_logloss: 0.162355#011train's binary_logloss: 0.133383\u001b[0m\n",
      "\u001b[35m[90]#011val's binary_logloss: 0.162387#011train's binary_logloss: 0.132846\u001b[0m\n",
      "\u001b[35m[91]#011val's binary_logloss: 0.162324#011train's binary_logloss: 0.132478\u001b[0m\n",
      "\u001b[35m[92]#011val's binary_logloss: 0.162308#011train's binary_logloss: 0.131873\u001b[0m\n",
      "\u001b[35m[93]#011val's binary_logloss: 0.162259#011train's binary_logloss: 0.131415\u001b[0m\n",
      "\u001b[35m[94]#011val's binary_logloss: 0.162257#011train's binary_logloss: 0.130902\u001b[0m\n",
      "\u001b[35m[95]#011val's binary_logloss: 0.16211#011train's binary_logloss: 0.130493\u001b[0m\n",
      "\u001b[35m[96]#011val's binary_logloss: 0.161996#011train's binary_logloss: 0.129924\u001b[0m\n",
      "\u001b[35m[97]#011val's binary_logloss: 0.161912#011train's binary_logloss: 0.129514\u001b[0m\n",
      "\u001b[35m[98]#011val's binary_logloss: 0.161793#011train's binary_logloss: 0.128921\u001b[0m\n",
      "\u001b[34m[90]#011val's binary_logloss: 0.191418#011train's binary_logloss: 0.123277\u001b[0m\n",
      "\u001b[34m[91]#011val's binary_logloss: 0.191432#011train's binary_logloss: 0.122963\u001b[0m\n",
      "\u001b[34m[92]#011val's binary_logloss: 0.191445#011train's binary_logloss: 0.122443\u001b[0m\n",
      "\u001b[34m[93]#011val's binary_logloss: 0.191506#011train's binary_logloss: 0.122007\u001b[0m\n",
      "\u001b[34m[94]#011val's binary_logloss: 0.191526#011train's binary_logloss: 0.121526\u001b[0m\n",
      "\u001b[34m[95]#011val's binary_logloss: 0.191439#011train's binary_logloss: 0.12112\u001b[0m\n",
      "\u001b[34m[96]#011val's binary_logloss: 0.191343#011train's binary_logloss: 0.120679\u001b[0m\n",
      "\u001b[34m[97]#011val's binary_logloss: 0.191296#011train's binary_logloss: 0.120376\u001b[0m\n",
      "\u001b[34m[98]#011val's binary_logloss: 0.19121#011train's binary_logloss: 0.119875\u001b[0m\n",
      "\u001b[34m[99]#011val's binary_logloss: 0.191086#011train's binary_logloss: 0.119369\u001b[0m\n",
      "\u001b[34m[100]#011val's binary_logloss: 0.190914#011train's binary_logloss: 0.118976\u001b[0m\n",
      "\u001b[35m[99]#011val's binary_logloss: 0.161741#011train's binary_logloss: 0.128335\u001b[0m\n",
      "\u001b[35m[100]#011val's binary_logloss: 0.161782#011train's binary_logloss: 0.127834\u001b[0m\n",
      "\u001b[35m[101]#011val's binary_logloss: 0.161676#011train's binary_logloss: 0.127345\u001b[0m\n",
      "\u001b[34m[101]#011val's binary_logloss: 0.190886#011train's binary_logloss: 0.118656\u001b[0m\n",
      "\u001b[34m[102]#011val's binary_logloss: 0.190877#011train's binary_logloss: 0.118309\u001b[0m\n",
      "\u001b[34m[103]#011val's binary_logloss: 0.190888#011train's binary_logloss: 0.117922\u001b[0m\n",
      "\u001b[35m[102]#011val's binary_logloss: 0.161645#011train's binary_logloss: 0.126865\u001b[0m\n",
      "\u001b[35m[103]#011val's binary_logloss: 0.161547#011train's binary_logloss: 0.126436\u001b[0m\n",
      "\u001b[34m[104]#011val's binary_logloss: 0.190997#011train's binary_logloss: 0.117581\u001b[0m\n",
      "\u001b[34m[105]#011val's binary_logloss: 0.190972#011train's binary_logloss: 0.117207\u001b[0m\n",
      "\u001b[35m[104]#011val's binary_logloss: 0.16158#011train's binary_logloss: 0.125932\u001b[0m\n",
      "\u001b[35m[105]#011val's binary_logloss: 0.161438#011train's binary_logloss: 0.125474\u001b[0m\n",
      "\u001b[34m[106]#011val's binary_logloss: 0.191037#011train's binary_logloss: 0.116766\u001b[0m\n",
      "\u001b[34m[107]#011val's binary_logloss: 0.190998#011train's binary_logloss: 0.116338\u001b[0m\n",
      "\u001b[35m[106]#011val's binary_logloss: 0.161376#011train's binary_logloss: 0.125019\u001b[0m\n",
      "\u001b[35m[107]#011val's binary_logloss: 0.161386#011train's binary_logloss: 0.12458\u001b[0m\n",
      "\u001b[35m[108]#011val's binary_logloss: 0.16132#011train's binary_logloss: 0.124095\u001b[0m\n",
      "\u001b[34m[108]#011val's binary_logloss: 0.190915#011train's binary_logloss: 0.115932\u001b[0m\n",
      "\u001b[34m[109]#011val's binary_logloss: 0.190906#011train's binary_logloss: 0.115498\u001b[0m\n",
      "\u001b[34m[110]#011val's binary_logloss: 0.19085#011train's binary_logloss: 0.115176\u001b[0m\n",
      "\u001b[35m[109]#011val's binary_logloss: 0.161267#011train's binary_logloss: 0.123622\u001b[0m\n",
      "\u001b[35m[110]#011val's binary_logloss: 0.161209#011train's binary_logloss: 0.123213\u001b[0m\n",
      "\u001b[35m[111]#011val's binary_logloss: 0.161226#011train's binary_logloss: 0.122707\u001b[0m\n",
      "\u001b[34m[111]#011val's binary_logloss: 0.190834#011train's binary_logloss: 0.114713\u001b[0m\n",
      "\u001b[34m[112]#011val's binary_logloss: 0.190744#011train's binary_logloss: 0.114216\u001b[0m\n",
      "\u001b[34m[113]#011val's binary_logloss: 0.190617#011train's binary_logloss: 0.113845\u001b[0m\n",
      "\u001b[35m[112]#011val's binary_logloss: 0.16124#011train's binary_logloss: 0.122152\u001b[0m\n",
      "\u001b[35m[113]#011val's binary_logloss: 0.161138#011train's binary_logloss: 0.12177\u001b[0m\n",
      "\u001b[35m[114]#011val's binary_logloss: 0.161042#011train's binary_logloss: 0.121299\u001b[0m\n",
      "\u001b[34m[114]#011val's binary_logloss: 0.190617#011train's binary_logloss: 0.11338\u001b[0m\n",
      "\u001b[34m[115]#011val's binary_logloss: 0.190533#011train's binary_logloss: 0.112895\u001b[0m\n",
      "\u001b[34m[116]#011val's binary_logloss: 0.190468#011train's binary_logloss: 0.112464\u001b[0m\n",
      "\u001b[35m[115]#011val's binary_logloss: 0.161013#011train's binary_logloss: 0.120794\u001b[0m\n",
      "\u001b[35m[116]#011val's binary_logloss: 0.160992#011train's binary_logloss: 0.12035\u001b[0m\n",
      "\u001b[35m[117]#011val's binary_logloss: 0.160916#011train's binary_logloss: 0.119864\u001b[0m\n",
      "\u001b[35m[118]#011val's binary_logloss: 0.160873#011train's binary_logloss: 0.119533\u001b[0m\n",
      "\u001b[35m[119]#011val's binary_logloss: 0.160846#011train's binary_logloss: 0.119116\u001b[0m\n",
      "\u001b[35m[120]#011val's binary_logloss: 0.160849#011train's binary_logloss: 0.118715\u001b[0m\n",
      "\u001b[35m[121]#011val's binary_logloss: 0.160838#011train's binary_logloss: 0.118315\u001b[0m\n",
      "\u001b[35m[122]#011val's binary_logloss: 0.16075#011train's binary_logloss: 0.117912\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[123]#011val's binary_logloss: 0.160711#011train's binary_logloss: 0.117671\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[124]#011val's binary_logloss: 0.160739#011train's binary_logloss: 0.117401\u001b[0m\n",
      "\u001b[35m[125]#011val's binary_logloss: 0.160681#011train's binary_logloss: 0.117109\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[126]#011val's binary_logloss: 0.16065#011train's binary_logloss: 0.116859\u001b[0m\n",
      "\u001b[34m[117]#011val's binary_logloss: 0.190406#011train's binary_logloss: 0.11204\u001b[0m\n",
      "\u001b[34m[118]#011val's binary_logloss: 0.190378#011train's binary_logloss: 0.111737\u001b[0m\n",
      "\u001b[34m[119]#011val's binary_logloss: 0.190234#011train's binary_logloss: 0.111347\u001b[0m\n",
      "\u001b[34m[120]#011val's binary_logloss: 0.190229#011train's binary_logloss: 0.11094\u001b[0m\n",
      "\u001b[34m[121]#011val's binary_logloss: 0.190199#011train's binary_logloss: 0.110589\u001b[0m\n",
      "\u001b[34m[122]#011val's binary_logloss: 0.190191#011train's binary_logloss: 0.11025\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[123]#011val's binary_logloss: 0.190096#011train's binary_logloss: 0.110053\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[124]#011val's binary_logloss: 0.189999#011train's binary_logloss: 0.109839\u001b[0m\n",
      "\u001b[34m[125]#011val's binary_logloss: 0.189929#011train's binary_logloss: 0.109542\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[126]#011val's binary_logloss: 0.189934#011train's binary_logloss: 0.109263\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[127]#011val's binary_logloss: 0.189958#011train's binary_logloss: 0.108873\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[128]#011val's binary_logloss: 0.189844#011train's binary_logloss: 0.108431\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[129]#011val's binary_logloss: 0.189848#011train's binary_logloss: 0.108177\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[127]#011val's binary_logloss: 0.160627#011train's binary_logloss: 0.116524\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[128]#011val's binary_logloss: 0.16056#011train's binary_logloss: 0.116202\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[129]#011val's binary_logloss: 0.160528#011train's binary_logloss: 0.116006\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[130]#011val's binary_logloss: 0.160444#011train's binary_logloss: 0.115791\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[130]#011val's binary_logloss: 0.189741#011train's binary_logloss: 0.107896\u001b[0m\n",
      "\u001b[34m[131]#011val's binary_logloss: 0.189588#011train's binary_logloss: 0.107614\u001b[0m\n",
      "\u001b[34m[132]#011val's binary_logloss: 0.189506#011train's binary_logloss: 0.10734\u001b[0m\n",
      "\u001b[35m[131]#011val's binary_logloss: 0.160429#011train's binary_logloss: 0.115459\u001b[0m\n",
      "\u001b[35m[132]#011val's binary_logloss: 0.160413#011train's binary_logloss: 0.115156\u001b[0m\n",
      "\u001b[35m[133]#011val's binary_logloss: 0.160379#011train's binary_logloss: 0.114664\u001b[0m\n",
      "\u001b[34m[133]#011val's binary_logloss: 0.189404#011train's binary_logloss: 0.107031\u001b[0m\n",
      "\u001b[34m[134]#011val's binary_logloss: 0.189343#011train's binary_logloss: 0.106721\u001b[0m\n",
      "\u001b[34m[135]#011val's binary_logloss: 0.189292#011train's binary_logloss: 0.10649\u001b[0m\n",
      "\u001b[35m[134]#011val's binary_logloss: 0.160399#011train's binary_logloss: 0.114273\u001b[0m\n",
      "\u001b[35m[135]#011val's binary_logloss: 0.160343#011train's binary_logloss: 0.113954\u001b[0m\n",
      "\u001b[35m[136]#011val's binary_logloss: 0.160397#011train's binary_logloss: 0.113512\u001b[0m\n",
      "\u001b[34m[136]#011val's binary_logloss: 0.189269#011train's binary_logloss: 0.105969\u001b[0m\n",
      "\u001b[34m[137]#011val's binary_logloss: 0.189234#011train's binary_logloss: 0.105562\u001b[0m\n",
      "\u001b[34m[138]#011val's binary_logloss: 0.18915#011train's binary_logloss: 0.105119\u001b[0m\n",
      "\u001b[35m[137]#011val's binary_logloss: 0.160376#011train's binary_logloss: 0.113023\u001b[0m\n",
      "\u001b[35m[138]#011val's binary_logloss: 0.160406#011train's binary_logloss: 0.112543\u001b[0m\n",
      "\u001b[35m[139]#011val's binary_logloss: 0.160412#011train's binary_logloss: 0.11211\u001b[0m\n",
      "\u001b[34m[139]#011val's binary_logloss: 0.189156#011train's binary_logloss: 0.104678\u001b[0m\n",
      "\u001b[34m[140]#011val's binary_logloss: 0.18911#011train's binary_logloss: 0.104273\u001b[0m\n",
      "\u001b[34m[141]#011val's binary_logloss: 0.189043#011train's binary_logloss: 0.103954\u001b[0m\n",
      "\u001b[35m[140]#011val's binary_logloss: 0.160355#011train's binary_logloss: 0.11166\u001b[0m\n",
      "\u001b[35m[141]#011val's binary_logloss: 0.160372#011train's binary_logloss: 0.111262\u001b[0m\n",
      "\u001b[35m[142]#011val's binary_logloss: 0.160354#011train's binary_logloss: 0.110897\u001b[0m\n",
      "\u001b[34m[142]#011val's binary_logloss: 0.188974#011train's binary_logloss: 0.103608\u001b[0m\n",
      "\u001b[34m[143]#011val's binary_logloss: 0.188881#011train's binary_logloss: 0.103394\u001b[0m\n",
      "\u001b[34m[144]#011val's binary_logloss: 0.188825#011train's binary_logloss: 0.103029\u001b[0m\n",
      "\u001b[34m[145]#011val's binary_logloss: 0.188902#011train's binary_logloss: 0.102632\u001b[0m\n",
      "\u001b[35m[143]#011val's binary_logloss: 0.160346#011train's binary_logloss: 0.110653\u001b[0m\n",
      "\u001b[35m[144]#011val's binary_logloss: 0.160349#011train's binary_logloss: 0.110205\u001b[0m\n",
      "\u001b[35m[145]#011val's binary_logloss: 0.16035#011train's binary_logloss: 0.109791\u001b[0m\n",
      "\u001b[35m[146]#011val's binary_logloss: 0.160352#011train's binary_logloss: 0.10941\u001b[0m\n",
      "\u001b[34m[146]#011val's binary_logloss: 0.188838#011train's binary_logloss: 0.102353\u001b[0m\n",
      "\u001b[34m[147]#011val's binary_logloss: 0.188731#011train's binary_logloss: 0.101971\u001b[0m\n",
      "\u001b[34m[148]#011val's binary_logloss: 0.188572#011train's binary_logloss: 0.101602\u001b[0m\n",
      "\u001b[35m[147]#011val's binary_logloss: 0.160369#011train's binary_logloss: 0.109006\u001b[0m\n",
      "\u001b[35m[148]#011val's binary_logloss: 0.160382#011train's binary_logloss: 0.108578\u001b[0m\n",
      "\u001b[35m[149]#011val's binary_logloss: 0.160355#011train's binary_logloss: 0.108128\u001b[0m\n",
      "\u001b[34m[149]#011val's binary_logloss: 0.188478#011train's binary_logloss: 0.101275\u001b[0m\n",
      "\u001b[34m[150]#011val's binary_logloss: 0.188388#011train's binary_logloss: 0.100914\u001b[0m\n",
      "\u001b[34m[151]#011val's binary_logloss: 0.188318#011train's binary_logloss: 0.100632\u001b[0m\n",
      "\u001b[34m[152]#011val's binary_logloss: 0.188365#011train's binary_logloss: 0.100272\u001b[0m\n",
      "\u001b[35m[150]#011val's binary_logloss: 0.160344#011train's binary_logloss: 0.107742\u001b[0m\n",
      "\u001b[35m[151]#011val's binary_logloss: 0.16028#011train's binary_logloss: 0.107376\u001b[0m\n",
      "\u001b[35m[152]#011val's binary_logloss: 0.160217#011train's binary_logloss: 0.106961\u001b[0m\n",
      "\u001b[35m[153]#011val's binary_logloss: 0.160103#011train's binary_logloss: 0.106639\u001b[0m\n",
      "\u001b[34m[153]#011val's binary_logloss: 0.188403#011train's binary_logloss: 0.0999984\u001b[0m\n",
      "\u001b[34m[154]#011val's binary_logloss: 0.188419#011train's binary_logloss: 0.0996467\u001b[0m\n",
      "\u001b[34m[155]#011val's binary_logloss: 0.188419#011train's binary_logloss: 0.0993077\u001b[0m\n",
      "\u001b[35m[154]#011val's binary_logloss: 0.160054#011train's binary_logloss: 0.106317\u001b[0m\n",
      "\u001b[35m[155]#011val's binary_logloss: 0.159969#011train's binary_logloss: 0.105942\u001b[0m\n",
      "\u001b[34m[156]#011val's binary_logloss: 0.188384#011train's binary_logloss: 0.0988851\u001b[0m\n",
      "\u001b[34m[157]#011val's binary_logloss: 0.188434#011train's binary_logloss: 0.0984883\u001b[0m\n",
      "\u001b[35m[156]#011val's binary_logloss: 0.159882#011train's binary_logloss: 0.10558\u001b[0m\n",
      "\u001b[35m[157]#011val's binary_logloss: 0.159982#011train's binary_logloss: 0.105219\u001b[0m\n",
      "\u001b[34m[158]#011val's binary_logloss: 0.188388#011train's binary_logloss: 0.0981066\u001b[0m\n",
      "\u001b[34m[159]#011val's binary_logloss: 0.188319#011train's binary_logloss: 0.0977281\u001b[0m\n",
      "\u001b[34m[160]#011val's binary_logloss: 0.188306#011train's binary_logloss: 0.0973263\u001b[0m\n",
      "\u001b[34m[161]#011val's binary_logloss: 0.188106#011train's binary_logloss: 0.0969204\u001b[0m\n",
      "\u001b[34m[162]#011val's binary_logloss: 0.188046#011train's binary_logloss: 0.0965292\u001b[0m\n",
      "\u001b[35m[158]#011val's binary_logloss: 0.159985#011train's binary_logloss: 0.10489\u001b[0m\n",
      "\u001b[35m[159]#011val's binary_logloss: 0.159975#011train's binary_logloss: 0.104507\u001b[0m\n",
      "\u001b[35m[160]#011val's binary_logloss: 0.160015#011train's binary_logloss: 0.104125\u001b[0m\n",
      "\u001b[35m[161]#011val's binary_logloss: 0.159962#011train's binary_logloss: 0.103737\u001b[0m\n",
      "\u001b[35m[162]#011val's binary_logloss: 0.159927#011train's binary_logloss: 0.103373\u001b[0m\n",
      "\u001b[35m[163]#011val's binary_logloss: 0.159938#011train's binary_logloss: 0.102983\u001b[0m\n",
      "\u001b[34m[163]#011val's binary_logloss: 0.188045#011train's binary_logloss: 0.0961162\u001b[0m\n",
      "\u001b[34m[164]#011val's binary_logloss: 0.188032#011train's binary_logloss: 0.0956975\u001b[0m\n",
      "\u001b[34m[165]#011val's binary_logloss: 0.188013#011train's binary_logloss: 0.0953111\u001b[0m\n",
      "\u001b[35m[164]#011val's binary_logloss: 0.159867#011train's binary_logloss: 0.102615\u001b[0m\n",
      "\u001b[35m[165]#011val's binary_logloss: 0.159826#011train's binary_logloss: 0.102245\u001b[0m\n",
      "\u001b[34m[166]#011val's binary_logloss: 0.188068#011train's binary_logloss: 0.094954\u001b[0m\n",
      "\u001b[34m[167]#011val's binary_logloss: 0.188109#011train's binary_logloss: 0.0946696\u001b[0m\n",
      "\u001b[35m[166]#011val's binary_logloss: 0.159787#011train's binary_logloss: 0.101837\u001b[0m\n",
      "\u001b[35m[167]#011val's binary_logloss: 0.159755#011train's binary_logloss: 0.101526\u001b[0m\n",
      "\u001b[35m[168]#011val's binary_logloss: 0.159755#011train's binary_logloss: 0.101232\u001b[0m\n",
      "\u001b[34m[168]#011val's binary_logloss: 0.188117#011train's binary_logloss: 0.0944103\u001b[0m\n",
      "\u001b[34m[169]#011val's binary_logloss: 0.188073#011train's binary_logloss: 0.0941276\u001b[0m\n",
      "\u001b[34m[170]#011val's binary_logloss: 0.188065#011train's binary_logloss: 0.0937793\u001b[0m\n",
      "\u001b[35m[169]#011val's binary_logloss: 0.159709#011train's binary_logloss: 0.100933\u001b[0m\n",
      "\u001b[35m[170]#011val's binary_logloss: 0.159685#011train's binary_logloss: 0.100547\u001b[0m\n",
      "\u001b[34m[171]#011val's binary_logloss: 0.188028#011train's binary_logloss: 0.0935559\u001b[0m\n",
      "\u001b[34m[172]#011val's binary_logloss: 0.187961#011train's binary_logloss: 0.0932902\u001b[0m\n",
      "\u001b[34m[173]#011val's binary_logloss: 0.187853#011train's binary_logloss: 0.0930561\u001b[0m\n",
      "\u001b[35m[171]#011val's binary_logloss: 0.159665#011train's binary_logloss: 0.100285\u001b[0m\n",
      "\u001b[35m[172]#011val's binary_logloss: 0.159544#011train's binary_logloss: 0.0999912\u001b[0m\n",
      "\u001b[35m[173]#011val's binary_logloss: 0.159556#011train's binary_logloss: 0.0997543\u001b[0m\n",
      "\u001b[34m[174]#011val's binary_logloss: 0.187769#011train's binary_logloss: 0.0928086\u001b[0m\n",
      "\u001b[34m[175]#011val's binary_logloss: 0.187645#011train's binary_logloss: 0.0925826\u001b[0m\n",
      "\u001b[35m[174]#011val's binary_logloss: 0.159542#011train's binary_logloss: 0.0994368\u001b[0m\n",
      "\u001b[35m[175]#011val's binary_logloss: 0.159484#011train's binary_logloss: 0.0991884\u001b[0m\n",
      "\u001b[35m[176]#011val's binary_logloss: 0.159469#011train's binary_logloss: 0.0988664\u001b[0m\n",
      "\u001b[34m[176]#011val's binary_logloss: 0.18766#011train's binary_logloss: 0.0922631\u001b[0m\n",
      "\u001b[34m[177]#011val's binary_logloss: 0.187615#011train's binary_logloss: 0.0919465\u001b[0m\n",
      "\u001b[34m[178]#011val's binary_logloss: 0.18766#011train's binary_logloss: 0.0916172\u001b[0m\n",
      "\u001b[35m[177]#011val's binary_logloss: 0.159429#011train's binary_logloss: 0.0985345\u001b[0m\n",
      "\u001b[35m[178]#011val's binary_logloss: 0.159383#011train's binary_logloss: 0.0981598\u001b[0m\n",
      "\u001b[35m[179]#011val's binary_logloss: 0.159319#011train's binary_logloss: 0.0978221\u001b[0m\n",
      "\u001b[34m[179]#011val's binary_logloss: 0.187731#011train's binary_logloss: 0.0913292\u001b[0m\n",
      "\u001b[34m[180]#011val's binary_logloss: 0.187737#011train's binary_logloss: 0.0910665\u001b[0m\n",
      "\u001b[34m[181]#011val's binary_logloss: 0.187779#011train's binary_logloss: 0.0907006\u001b[0m\n",
      "\u001b[35m[180]#011val's binary_logloss: 0.1593#011train's binary_logloss: 0.0975578\u001b[0m\n",
      "\u001b[35m[181]#011val's binary_logloss: 0.159328#011train's binary_logloss: 0.0971603\u001b[0m\n",
      "\u001b[34m[182]#011val's binary_logloss: 0.187806#011train's binary_logloss: 0.0903776\u001b[0m\n",
      "\u001b[34m[183]#011val's binary_logloss: 0.187825#011train's binary_logloss: 0.0900451\u001b[0m\n",
      "\u001b[34m[184]#011val's binary_logloss: 0.187762#011train's binary_logloss: 0.0897331\u001b[0m\n",
      "\u001b[35m[182]#011val's binary_logloss: 0.159345#011train's binary_logloss: 0.0967888\u001b[0m\n",
      "\u001b[35m[183]#011val's binary_logloss: 0.159299#011train's binary_logloss: 0.0964177\u001b[0m\n",
      "\u001b[35m[184]#011val's binary_logloss: 0.159279#011train's binary_logloss: 0.096059\u001b[0m\n",
      "\u001b[34m[185]#011val's binary_logloss: 0.187688#011train's binary_logloss: 0.0893814\u001b[0m\n",
      "\u001b[34m[186]#011val's binary_logloss: 0.187767#011train's binary_logloss: 0.0891111\u001b[0m\n",
      "\u001b[34m[187]#011val's binary_logloss: 0.187741#011train's binary_logloss: 0.0887578\u001b[0m\n",
      "\u001b[35m[185]#011val's binary_logloss: 0.159296#011train's binary_logloss: 0.0956516\u001b[0m\n",
      "\u001b[35m[186]#011val's binary_logloss: 0.159297#011train's binary_logloss: 0.0952902\u001b[0m\n",
      "\u001b[35m[187]#011val's binary_logloss: 0.159274#011train's binary_logloss: 0.0949142\u001b[0m\n",
      "\u001b[35m[188]#011val's binary_logloss: 0.159239#011train's binary_logloss: 0.0945598\u001b[0m\n",
      "\u001b[34m[188]#011val's binary_logloss: 0.187737#011train's binary_logloss: 0.0883977\u001b[0m\n",
      "\u001b[34m[189]#011val's binary_logloss: 0.187772#011train's binary_logloss: 0.0880755\u001b[0m\n",
      "\u001b[34m[190]#011val's binary_logloss: 0.187787#011train's binary_logloss: 0.0877752\u001b[0m\n",
      "\u001b[35m[189]#011val's binary_logloss: 0.159276#011train's binary_logloss: 0.0941898\u001b[0m\n",
      "\u001b[35m[190]#011val's binary_logloss: 0.159267#011train's binary_logloss: 0.0938257\u001b[0m\n",
      "\u001b[34m[191]#011val's binary_logloss: 0.187708#011train's binary_logloss: 0.0874949\u001b[0m\n",
      "\u001b[34m[192]#011val's binary_logloss: 0.187785#011train's binary_logloss: 0.0871635\u001b[0m\n",
      "\u001b[34m[193]#011val's binary_logloss: 0.187737#011train's binary_logloss: 0.0868842\u001b[0m\n",
      "\u001b[35m[191]#011val's binary_logloss: 0.159236#011train's binary_logloss: 0.0934838\u001b[0m\n",
      "\u001b[35m[192]#011val's binary_logloss: 0.159263#011train's binary_logloss: 0.093156\u001b[0m\n",
      "\u001b[35m[193]#011val's binary_logloss: 0.159244#011train's binary_logloss: 0.0928359\u001b[0m\n",
      "\u001b[34m[194]#011val's binary_logloss: 0.187683#011train's binary_logloss: 0.086582\u001b[0m\n",
      "\u001b[34m[195]#011val's binary_logloss: 0.187683#011train's binary_logloss: 0.086294\u001b[0m\n",
      "\u001b[34m[196]#011val's binary_logloss: 0.187649#011train's binary_logloss: 0.0860352\u001b[0m\n",
      "\u001b[35m[194]#011val's binary_logloss: 0.159208#011train's binary_logloss: 0.0925394\u001b[0m\n",
      "\u001b[35m[195]#011val's binary_logloss: 0.159231#011train's binary_logloss: 0.0922188\u001b[0m\n",
      "\u001b[35m[196]#011val's binary_logloss: 0.159276#011train's binary_logloss: 0.0919532\u001b[0m\n",
      "\u001b[34m[197]#011val's binary_logloss: 0.187593#011train's binary_logloss: 0.0857319\u001b[0m\n",
      "\u001b[34m[198]#011val's binary_logloss: 0.187489#011train's binary_logloss: 0.0855048\u001b[0m\n",
      "\u001b[34m[199]#011val's binary_logloss: 0.187396#011train's binary_logloss: 0.0851761\u001b[0m\n",
      "\u001b[35m[197]#011val's binary_logloss: 0.159312#011train's binary_logloss: 0.0916532\u001b[0m\n",
      "\u001b[35m[198]#011val's binary_logloss: 0.159341#011train's binary_logloss: 0.0914378\u001b[0m\n",
      "\u001b[35m[199]#011val's binary_logloss: 0.159403#011train's binary_logloss: 0.091114\u001b[0m\n",
      "\u001b[35m[200]#011val's binary_logloss: 0.159442#011train's binary_logloss: 0.0908463\u001b[0m\n",
      "\u001b[34m[200]#011val's binary_logloss: 0.187423#011train's binary_logloss: 0.0848971\u001b[0m\n",
      "\u001b[34m[201]#011val's binary_logloss: 0.187437#011train's binary_logloss: 0.0846046\u001b[0m\n",
      "\u001b[34m[202]#011val's binary_logloss: 0.187448#011train's binary_logloss: 0.0843092\u001b[0m\n",
      "\u001b[35m[201]#011val's binary_logloss: 0.15948#011train's binary_logloss: 0.0904902\u001b[0m\n",
      "\u001b[35m[202]#011val's binary_logloss: 0.15955#011train's binary_logloss: 0.0901581\u001b[0m\n",
      "\u001b[34m[203]#011val's binary_logloss: 0.187318#011train's binary_logloss: 0.0840253\u001b[0m\n",
      "\u001b[34m[204]#011val's binary_logloss: 0.187276#011train's binary_logloss: 0.0837163\u001b[0m\n",
      "\u001b[34m[205]#011val's binary_logloss: 0.187258#011train's binary_logloss: 0.0834202\u001b[0m\n",
      "\u001b[35m[203]#011val's binary_logloss: 0.159514#011train's binary_logloss: 0.0898323\u001b[0m\n",
      "\u001b[35m[204]#011val's binary_logloss: 0.159515#011train's binary_logloss: 0.0895289\u001b[0m\n",
      "\u001b[35m[205]#011val's binary_logloss: 0.159478#011train's binary_logloss: 0.0892141\u001b[0m\n",
      "\u001b[34m[206]#011val's binary_logloss: 0.187272#011train's binary_logloss: 0.0831588\u001b[0m\n",
      "\u001b[34m[207]#011val's binary_logloss: 0.18732#011train's binary_logloss: 0.0829052\u001b[0m\n",
      "\u001b[34m[208]#011val's binary_logloss: 0.187292#011train's binary_logloss: 0.0826239\u001b[0m\n",
      "\u001b[35m[206]#011val's binary_logloss: 0.159499#011train's binary_logloss: 0.0889018\u001b[0m\n",
      "\u001b[35m[207]#011val's binary_logloss: 0.159528#011train's binary_logloss: 0.088567\u001b[0m\n",
      "\u001b[35m[208]#011val's binary_logloss: 0.159576#011train's binary_logloss: 0.0882237\u001b[0m\n",
      "\u001b[34m[209]#011val's binary_logloss: 0.187456#011train's binary_logloss: 0.0823502\u001b[0m\n",
      "\u001b[34m[210]#011val's binary_logloss: 0.187375#011train's binary_logloss: 0.0820445\u001b[0m\n",
      "\u001b[34m[211]#011val's binary_logloss: 0.187423#011train's binary_logloss: 0.0817982\u001b[0m\n",
      "\u001b[35m[209]#011val's binary_logloss: 0.159634#011train's binary_logloss: 0.0879001\u001b[0m\n",
      "\u001b[35m[210]#011val's binary_logloss: 0.159631#011train's binary_logloss: 0.087576\u001b[0m\n",
      "\u001b[35m[211]#011val's binary_logloss: 0.159644#011train's binary_logloss: 0.0872794\u001b[0m\n",
      "\u001b[34m[212]#011val's binary_logloss: 0.1874#011train's binary_logloss: 0.0815094\u001b[0m\n",
      "\u001b[34m[213]#011val's binary_logloss: 0.187384#011train's binary_logloss: 0.0812836\u001b[0m\n",
      "\u001b[34m[214]#011val's binary_logloss: 0.187411#011train's binary_logloss: 0.0810095\u001b[0m\n",
      "\u001b[35m[212]#011val's binary_logloss: 0.159655#011train's binary_logloss: 0.0869389\u001b[0m\n",
      "\u001b[35m[213]#011val's binary_logloss: 0.159637#011train's binary_logloss: 0.0866929\u001b[0m\n",
      "\u001b[35m[214]#011val's binary_logloss: 0.159682#011train's binary_logloss: 0.0863915\u001b[0m\n",
      "\u001b[34m[215]#011val's binary_logloss: 0.187516#011train's binary_logloss: 0.0807883\u001b[0m\n",
      "\u001b[34m[216]#011val's binary_logloss: 0.187515#011train's binary_logloss: 0.0804648\u001b[0m\n",
      "\u001b[35m[215]#011val's binary_logloss: 0.1597#011train's binary_logloss: 0.086078\u001b[0m\n",
      "\u001b[35m[216]#011val's binary_logloss: 0.159652#011train's binary_logloss: 0.0857704\u001b[0m\n",
      "\u001b[35m[217]#011val's binary_logloss: 0.15968#011train's binary_logloss: 0.0854914\u001b[0m\n",
      "\u001b[34m[217]#011val's binary_logloss: 0.187537#011train's binary_logloss: 0.0801208\u001b[0m\n",
      "\u001b[34m[218]#011val's binary_logloss: 0.187437#011train's binary_logloss: 0.0797978\u001b[0m\n",
      "\u001b[34m[219]#011val's binary_logloss: 0.187475#011train's binary_logloss: 0.0794837\u001b[0m\n",
      "\u001b[35m[218]#011val's binary_logloss: 0.159689#011train's binary_logloss: 0.0852129\u001b[0m\n",
      "\u001b[35m[219]#011val's binary_logloss: 0.159694#011train's binary_logloss: 0.084946\u001b[0m\n",
      "\u001b[35m[220]#011val's binary_logloss: 0.159714#011train's binary_logloss: 0.084665\u001b[0m\n",
      "\u001b[34m[220]#011val's binary_logloss: 0.187284#011train's binary_logloss: 0.0792237\u001b[0m\n",
      "\u001b[34m[221]#011val's binary_logloss: 0.187217#011train's binary_logloss: 0.0790911\u001b[0m\n",
      "\u001b[34m[222]#011val's binary_logloss: 0.187174#011train's binary_logloss: 0.0788355\u001b[0m\n",
      "\u001b[35m[221]#011val's binary_logloss: 0.159704#011train's binary_logloss: 0.0845135\u001b[0m\n",
      "\u001b[35m[222]#011val's binary_logloss: 0.159767#011train's binary_logloss: 0.0842559\u001b[0m\n",
      "\u001b[35m[223]#011val's binary_logloss: 0.159703#011train's binary_logloss: 0.0839803\u001b[0m\n",
      "\u001b[34m[223]#011val's binary_logloss: 0.187189#011train's binary_logloss: 0.0785724\u001b[0m\n",
      "\u001b[34m[224]#011val's binary_logloss: 0.187263#011train's binary_logloss: 0.0782879\u001b[0m\n",
      "\u001b[34m[225]#011val's binary_logloss: 0.187232#011train's binary_logloss: 0.078063\u001b[0m\n",
      "\u001b[35m[224]#011val's binary_logloss: 0.159762#011train's binary_logloss: 0.083691\u001b[0m\n",
      "\u001b[35m[225]#011val's binary_logloss: 0.159786#011train's binary_logloss: 0.0834569\u001b[0m\n",
      "\u001b[35m[226]#011val's binary_logloss: 0.159768#011train's binary_logloss: 0.0832446\u001b[0m\n",
      "\u001b[34m[226]#011val's binary_logloss: 0.187243#011train's binary_logloss: 0.0778688\u001b[0m\n",
      "\u001b[34m[227]#011val's binary_logloss: 0.187281#011train's binary_logloss: 0.0776706\u001b[0m\n",
      "\u001b[34m[228]#011val's binary_logloss: 0.187379#011train's binary_logloss: 0.0774409\u001b[0m\n",
      "\u001b[35m[227]#011val's binary_logloss: 0.159758#011train's binary_logloss: 0.0830463\u001b[0m\n",
      "\u001b[35m[228]#011val's binary_logloss: 0.159826#011train's binary_logloss: 0.0827875\u001b[0m\n",
      "\u001b[34m[229]#011val's binary_logloss: 0.187356#011train's binary_logloss: 0.077253\u001b[0m\n",
      "\u001b[34m[230]#011val's binary_logloss: 0.187382#011train's binary_logloss: 0.0770079\u001b[0m\n",
      "\u001b[34m[231]#011val's binary_logloss: 0.187311#011train's binary_logloss: 0.0768064\u001b[0m\n",
      "\u001b[35m[229]#011val's binary_logloss: 0.159859#011train's binary_logloss: 0.0825903\u001b[0m\n",
      "\u001b[35m[230]#011val's binary_logloss: 0.159912#011train's binary_logloss: 0.0823249\u001b[0m\n",
      "\u001b[35m[231]#011val's binary_logloss: 0.159875#011train's binary_logloss: 0.0821717\u001b[0m\n",
      "\u001b[35m[232]#011val's binary_logloss: 0.159877#011train's binary_logloss: 0.0819721\u001b[0m\n",
      "\u001b[34m[232]#011val's binary_logloss: 0.187384#011train's binary_logloss: 0.0766098\u001b[0m\n",
      "\u001b[34m[233]#011val's binary_logloss: 0.187408#011train's binary_logloss: 0.0764151\u001b[0m\n",
      "\u001b[34m[234]#011val's binary_logloss: 0.187505#011train's binary_logloss: 0.0762231\u001b[0m\n",
      "\u001b[35m[233]#011val's binary_logloss: 0.159893#011train's binary_logloss: 0.0817789\u001b[0m\n",
      "\u001b[35m[234]#011val's binary_logloss: 0.159887#011train's binary_logloss: 0.0815845\u001b[0m\n",
      "\u001b[35m[235]#011val's binary_logloss: 0.159884#011train's binary_logloss: 0.0813391\u001b[0m\n",
      "\u001b[34m[235]#011val's binary_logloss: 0.187567#011train's binary_logloss: 0.075992\u001b[0m\n",
      "\u001b[34m[236]#011val's binary_logloss: 0.187487#011train's binary_logloss: 0.0757355\u001b[0m\n",
      "\u001b[34m[237]#011val's binary_logloss: 0.187441#011train's binary_logloss: 0.075506\u001b[0m\n",
      "\u001b[35m[236]#011val's binary_logloss: 0.159897#011train's binary_logloss: 0.0810503\u001b[0m\n",
      "\u001b[35m[237]#011val's binary_logloss: 0.159888#011train's binary_logloss: 0.0808163\u001b[0m\n",
      "\u001b[34m[238]#011val's binary_logloss: 0.187499#011train's binary_logloss: 0.0752509\u001b[0m\n",
      "\u001b[34m[239]#011val's binary_logloss: 0.187599#011train's binary_logloss: 0.0749776\u001b[0m\n",
      "\u001b[34m[240]#011val's binary_logloss: 0.187698#011train's binary_logloss: 0.0746982\u001b[0m\n",
      "\u001b[35m[238]#011val's binary_logloss: 0.159965#011train's binary_logloss: 0.0805445\u001b[0m\n",
      "\u001b[35m[239]#011val's binary_logloss: 0.159958#011train's binary_logloss: 0.0802736\u001b[0m\n",
      "\u001b[35m[240]#011val's binary_logloss: 0.159947#011train's binary_logloss: 0.080001\u001b[0m\n",
      "\u001b[34m[241]#011val's binary_logloss: 0.187716#011train's binary_logloss: 0.0744588\u001b[0m\n",
      "\u001b[34m[242]#011val's binary_logloss: 0.187647#011train's binary_logloss: 0.0742202\u001b[0m\n",
      "\u001b[35m[241]#011val's binary_logloss: 0.159919#011train's binary_logloss: 0.0797504\u001b[0m\n",
      "\u001b[35m[242]#011val's binary_logloss: 0.159972#011train's binary_logloss: 0.0795289\u001b[0m\n",
      "\u001b[34m[243]#011val's binary_logloss: 0.18775#011train's binary_logloss: 0.0739291\u001b[0m\n",
      "\u001b[34m[244]#011val's binary_logloss: 0.187738#011train's binary_logloss: 0.0736896\u001b[0m\n",
      "\u001b[34m[245]#011val's binary_logloss: 0.187761#011train's binary_logloss: 0.0734725\u001b[0m\n",
      "\u001b[35m[243]#011val's binary_logloss: 0.159977#011train's binary_logloss: 0.0792758\u001b[0m\n",
      "\u001b[35m[244]#011val's binary_logloss: 0.160027#011train's binary_logloss: 0.0790499\u001b[0m\n",
      "\u001b[35m[245]#011val's binary_logloss: 0.160066#011train's binary_logloss: 0.0788364\u001b[0m\n",
      "\u001b[35m[246]#011val's binary_logloss: 0.159964#011train's binary_logloss: 0.0786524\u001b[0m\n",
      "\u001b[34m[246]#011val's binary_logloss: 0.187768#011train's binary_logloss: 0.0733337\u001b[0m\n",
      "\u001b[34m[247]#011val's binary_logloss: 0.18781#011train's binary_logloss: 0.0731519\u001b[0m\n",
      "\u001b[34m[248]#011val's binary_logloss: 0.187895#011train's binary_logloss: 0.072935\u001b[0m\n",
      "\u001b[35m[247]#011val's binary_logloss: 0.159965#011train's binary_logloss: 0.0784752\u001b[0m\n",
      "\u001b[35m[248]#011val's binary_logloss: 0.159949#011train's binary_logloss: 0.0782557\u001b[0m\n",
      "\u001b[35m[249]#011val's binary_logloss: 0.159982#011train's binary_logloss: 0.0780438\u001b[0m\n",
      "\u001b[34m[249]#011val's binary_logloss: 0.187954#011train's binary_logloss: 0.0727087\u001b[0m\n",
      "\u001b[34m[250]#011val's binary_logloss: 0.187876#011train's binary_logloss: 0.0724937\u001b[0m\n",
      "\u001b[34m[251]#011val's binary_logloss: 0.18805#011train's binary_logloss: 0.0722035\u001b[0m\n",
      "\u001b[35m[250]#011val's binary_logloss: 0.160007#011train's binary_logloss: 0.077776\u001b[0m\n",
      "\u001b[35m[251]#011val's binary_logloss: 0.159999#011train's binary_logloss: 0.0774997\u001b[0m\n",
      "\u001b[34m[252]#011val's binary_logloss: 0.188044#011train's binary_logloss: 0.0719472\u001b[0m\n",
      "\u001b[34m[253]#011val's binary_logloss: 0.188124#011train's binary_logloss: 0.0717238\u001b[0m\n",
      "\u001b[35m[252]#011val's binary_logloss: 0.159989#011train's binary_logloss: 0.0772208\u001b[0m\n",
      "\u001b[35m[253]#011val's binary_logloss: 0.160099#011train's binary_logloss: 0.0769361\u001b[0m\n",
      "\u001b[35m[254]#011val's binary_logloss: 0.160111#011train's binary_logloss: 0.0766965\u001b[0m\n",
      "\u001b[34m[254]#011val's binary_logloss: 0.188198#011train's binary_logloss: 0.0714879\u001b[0m\n",
      "\u001b[34m[255]#011val's binary_logloss: 0.188246#011train's binary_logloss: 0.0712248\u001b[0m\n",
      "\u001b[34m[256]#011val's binary_logloss: 0.188309#011train's binary_logloss: 0.0709519\u001b[0m\n",
      "\u001b[35m[255]#011val's binary_logloss: 0.160116#011train's binary_logloss: 0.0764288\u001b[0m\n",
      "\u001b[35m[256]#011val's binary_logloss: 0.160164#011train's binary_logloss: 0.0761896\u001b[0m\n",
      "\u001b[35m[257]#011val's binary_logloss: 0.160186#011train's binary_logloss: 0.0759953\u001b[0m\n",
      "\u001b[34m[257]#011val's binary_logloss: 0.188282#011train's binary_logloss: 0.0707555\u001b[0m\n",
      "\u001b[34m[258]#011val's binary_logloss: 0.188227#011train's binary_logloss: 0.0705155\u001b[0m\n",
      "\u001b[34m[259]#011val's binary_logloss: 0.188236#011train's binary_logloss: 0.0702481\u001b[0m\n",
      "\u001b[35m[258]#011val's binary_logloss: 0.160202#011train's binary_logloss: 0.075748\u001b[0m\n",
      "\u001b[35m[259]#011val's binary_logloss: 0.160247#011train's binary_logloss: 0.0755115\u001b[0m\n",
      "\u001b[35m[260]#011val's binary_logloss: 0.160264#011train's binary_logloss: 0.0752859\u001b[0m\n",
      "\u001b[34m[260]#011val's binary_logloss: 0.188172#011train's binary_logloss: 0.0699987\u001b[0m\n",
      "\u001b[34m[261]#011val's binary_logloss: 0.188293#011train's binary_logloss: 0.0697703\u001b[0m\n",
      "\u001b[34m[262]#011val's binary_logloss: 0.188279#011train's binary_logloss: 0.0696075\u001b[0m\n",
      "\u001b[34m[263]#011val's binary_logloss: 0.188399#011train's binary_logloss: 0.0694047\u001b[0m\n",
      "\u001b[35m[261]#011val's binary_logloss: 0.160227#011train's binary_logloss: 0.0750283\u001b[0m\n",
      "\u001b[35m[262]#011val's binary_logloss: 0.160214#011train's binary_logloss: 0.0748255\u001b[0m\n",
      "\u001b[35m[263]#011val's binary_logloss: 0.160179#011train's binary_logloss: 0.0745833\u001b[0m\n",
      "\u001b[34m[264]#011val's binary_logloss: 0.188256#011train's binary_logloss: 0.0691859\u001b[0m\n",
      "\u001b[34m[265]#011val's binary_logloss: 0.188327#011train's binary_logloss: 0.0690068\u001b[0m\n",
      "\u001b[35m[264]#011val's binary_logloss: 0.160231#011train's binary_logloss: 0.0743179\u001b[0m\n",
      "\u001b[35m[265]#011val's binary_logloss: 0.160234#011train's binary_logloss: 0.0741003\u001b[0m\n",
      "\u001b[35m[266]#011val's binary_logloss: 0.160307#011train's binary_logloss: 0.0739483\u001b[0m\n",
      "\u001b[34m[266]#011val's binary_logloss: 0.18833#011train's binary_logloss: 0.0688915\u001b[0m\n",
      "\u001b[34m[267]#011val's binary_logloss: 0.188348#011train's binary_logloss: 0.068693\u001b[0m\n",
      "\u001b[34m[268]#011val's binary_logloss: 0.188397#011train's binary_logloss: 0.0684888\u001b[0m\n",
      "\u001b[35m[267]#011val's binary_logloss: 0.160337#011train's binary_logloss: 0.0737284\u001b[0m\n",
      "\u001b[35m[268]#011val's binary_logloss: 0.160338#011train's binary_logloss: 0.0734942\u001b[0m\n",
      "\u001b[35m[269]#011val's binary_logloss: 0.160404#011train's binary_logloss: 0.0733201\u001b[0m\n",
      "\u001b[34m[269]#011val's binary_logloss: 0.18848#011train's binary_logloss: 0.0683696\u001b[0m\n",
      "\u001b[34m[270]#011val's binary_logloss: 0.18844#011train's binary_logloss: 0.0682408\u001b[0m\n",
      "\u001b[34m[271]#011val's binary_logloss: 0.188522#011train's binary_logloss: 0.0680249\u001b[0m\n",
      "\u001b[35m[270]#011val's binary_logloss: 0.16042#011train's binary_logloss: 0.0731514\u001b[0m\n",
      "\u001b[35m[271]#011val's binary_logloss: 0.160446#011train's binary_logloss: 0.0729332\u001b[0m\n",
      "\u001b[34m[272]#011val's binary_logloss: 0.188515#011train's binary_logloss: 0.067787\u001b[0m\n",
      "\u001b[34m[273]#011val's binary_logloss: 0.18851#011train's binary_logloss: 0.06764\u001b[0m\n",
      "\u001b[34m[274]#011val's binary_logloss: 0.188488#011train's binary_logloss: 0.0673875\u001b[0m\n",
      "\u001b[35m[272]#011val's binary_logloss: 0.1605#011train's binary_logloss: 0.0727208\u001b[0m\n",
      "\u001b[35m[273]#011val's binary_logloss: 0.1605#011train's binary_logloss: 0.0725903\u001b[0m\n",
      "\u001b[35m[274]#011val's binary_logloss: 0.160523#011train's binary_logloss: 0.0723808\u001b[0m\n",
      "\u001b[34m[275]#011val's binary_logloss: 0.188557#011train's binary_logloss: 0.0672008\u001b[0m\n",
      "\u001b[34m[276]#011val's binary_logloss: 0.188568#011train's binary_logloss: 0.0670216\u001b[0m\n",
      "\u001b[34m[277]#011val's binary_logloss: 0.188618#011train's binary_logloss: 0.0668201\u001b[0m\n",
      "\u001b[35m[275]#011val's binary_logloss: 0.160555#011train's binary_logloss: 0.0722062\u001b[0m\n",
      "\u001b[35m[276]#011val's binary_logloss: 0.160605#011train's binary_logloss: 0.0720325\u001b[0m\n",
      "\u001b[35m[277]#011val's binary_logloss: 0.160564#011train's binary_logloss: 0.0718129\u001b[0m\n",
      "\u001b[34m[278]#011val's binary_logloss: 0.188716#011train's binary_logloss: 0.0666668\u001b[0m\n",
      "\u001b[34m[279]#011val's binary_logloss: 0.188681#011train's binary_logloss: 0.0664623\u001b[0m\n",
      "\u001b[35m[278]#011val's binary_logloss: 0.160613#011train's binary_logloss: 0.0716237\u001b[0m\n",
      "\u001b[35m[279]#011val's binary_logloss: 0.160655#011train's binary_logloss: 0.0713899\u001b[0m\n",
      "\u001b[35m[280]#011val's binary_logloss: 0.160685#011train's binary_logloss: 0.071229\u001b[0m\n",
      "\u001b[34m[280]#011val's binary_logloss: 0.188607#011train's binary_logloss: 0.0663152\u001b[0m\n",
      "\u001b[34m[281]#011val's binary_logloss: 0.188664#011train's binary_logloss: 0.0660765\u001b[0m\n",
      "\u001b[34m[282]#011val's binary_logloss: 0.188666#011train's binary_logloss: 0.0658739\u001b[0m\n",
      "\u001b[34m[283]#011val's binary_logloss: 0.188736#011train's binary_logloss: 0.0656635\u001b[0m\n",
      "\u001b[35m[281]#011val's binary_logloss: 0.160718#011train's binary_logloss: 0.0710147\u001b[0m\n",
      "\u001b[35m[282]#011val's binary_logloss: 0.160745#011train's binary_logloss: 0.0708327\u001b[0m\n",
      "\u001b[35m[283]#011val's binary_logloss: 0.160749#011train's binary_logloss: 0.0706551\u001b[0m\n",
      "\u001b[34m[284]#011val's binary_logloss: 0.188842#011train's binary_logloss: 0.0654784\u001b[0m\n",
      "\u001b[34m[285]#011val's binary_logloss: 0.188866#011train's binary_logloss: 0.0653096\u001b[0m\n",
      "\u001b[34m[286]#011val's binary_logloss: 0.18883#011train's binary_logloss: 0.0651066\u001b[0m\n",
      "\u001b[35m[284]#011val's binary_logloss: 0.160763#011train's binary_logloss: 0.070489\u001b[0m\n",
      "\u001b[35m[285]#011val's binary_logloss: 0.160789#011train's binary_logloss: 0.0703154\u001b[0m\n",
      "\u001b[35m[286]#011val's binary_logloss: 0.160809#011train's binary_logloss: 0.0701473\u001b[0m\n",
      "\u001b[35m[287]#011val's binary_logloss: 0.160791#011train's binary_logloss: 0.0699829\u001b[0m\n",
      "\u001b[34m[287]#011val's binary_logloss: 0.18884#011train's binary_logloss: 0.064911\u001b[0m\n",
      "\u001b[34m[288]#011val's binary_logloss: 0.188906#011train's binary_logloss: 0.0646961\u001b[0m\n",
      "\u001b[34m[289]#011val's binary_logloss: 0.188916#011train's binary_logloss: 0.0645009\u001b[0m\n",
      "\u001b[35m[288]#011val's binary_logloss: 0.160817#011train's binary_logloss: 0.0697614\u001b[0m\n",
      "\u001b[35m[289]#011val's binary_logloss: 0.160838#011train's binary_logloss: 0.0695958\u001b[0m\n",
      "\u001b[35m[290]#011val's binary_logloss: 0.160794#011train's binary_logloss: 0.0694486\u001b[0m\n",
      "\u001b[34m[290]#011val's binary_logloss: 0.188962#011train's binary_logloss: 0.0643553\u001b[0m\n",
      "\u001b[34m[291]#011val's binary_logloss: 0.189091#011train's binary_logloss: 0.0641628\u001b[0m\n",
      "\u001b[34m[292]#011val's binary_logloss: 0.189208#011train's binary_logloss: 0.0639539\u001b[0m\n",
      "\u001b[35m[291]#011val's binary_logloss: 0.160862#011train's binary_logloss: 0.0692019\u001b[0m\n",
      "\u001b[35m[292]#011val's binary_logloss: 0.160903#011train's binary_logloss: 0.0689376\u001b[0m\n",
      "\u001b[34m[293]#011val's binary_logloss: 0.189294#011train's binary_logloss: 0.0637558\u001b[0m\n",
      "\u001b[34m[294]#011val's binary_logloss: 0.189347#011train's binary_logloss: 0.0635661\u001b[0m\n",
      "\u001b[34m[295]#011val's binary_logloss: 0.189429#011train's binary_logloss: 0.0633658\u001b[0m\n",
      "\u001b[35m[293]#011val's binary_logloss: 0.161004#011train's binary_logloss: 0.0687041\u001b[0m\n",
      "\u001b[35m[294]#011val's binary_logloss: 0.161051#011train's binary_logloss: 0.0684449\u001b[0m\n",
      "\u001b[35m[295]#011val's binary_logloss: 0.161158#011train's binary_logloss: 0.0682097\u001b[0m\n",
      "\u001b[34m[296]#011val's binary_logloss: 0.18941#011train's binary_logloss: 0.0632093\u001b[0m\n",
      "\u001b[34m[297]#011val's binary_logloss: 0.189349#011train's binary_logloss: 0.0630656\u001b[0m\n",
      "\u001b[34m[298]#011val's binary_logloss: 0.189412#011train's binary_logloss: 0.0628874\u001b[0m\n",
      "\u001b[35m[296]#011val's binary_logloss: 0.161199#011train's binary_logloss: 0.0680206\u001b[0m\n",
      "\u001b[35m[297]#011val's binary_logloss: 0.161188#011train's binary_logloss: 0.0678592\u001b[0m\n",
      "\u001b[35m[298]#011val's binary_logloss: 0.161188#011train's binary_logloss: 0.0676602\u001b[0m\n",
      "\u001b[35m[299]#011val's binary_logloss: 0.161163#011train's binary_logloss: 0.0674828\u001b[0m\n",
      "\u001b[34m[299]#011val's binary_logloss: 0.189368#011train's binary_logloss: 0.062706\u001b[0m\n",
      "\u001b[34m[300]#011val's binary_logloss: 0.18943#011train's binary_logloss: 0.0625141\u001b[0m\n",
      "\u001b[34m[301]#011val's binary_logloss: 0.189406#011train's binary_logloss: 0.0623752\u001b[0m\n",
      "\u001b[35m[300]#011val's binary_logloss: 0.161185#011train's binary_logloss: 0.0673055\u001b[0m\n",
      "\u001b[35m[301]#011val's binary_logloss: 0.161262#011train's binary_logloss: 0.0671523\u001b[0m\n",
      "\u001b[35m[302]#011val's binary_logloss: 0.161324#011train's binary_logloss: 0.0669931\u001b[0m\n",
      "\u001b[34m[302]#011val's binary_logloss: 0.189354#011train's binary_logloss: 0.0622427\u001b[0m\n",
      "\u001b[34m[303]#011val's binary_logloss: 0.1893#011train's binary_logloss: 0.062089\u001b[0m\n",
      "\u001b[34m[304]#011val's binary_logloss: 0.189322#011train's binary_logloss: 0.0619802\u001b[0m\n",
      "\u001b[35m[303]#011val's binary_logloss: 0.161357#011train's binary_logloss: 0.0668359\u001b[0m\n",
      "\u001b[35m[304]#011val's binary_logloss: 0.16139#011train's binary_logloss: 0.0666894\u001b[0m\n",
      "\u001b[35m[305]#011val's binary_logloss: 0.161448#011train's binary_logloss: 0.0665411\u001b[0m\n",
      "\u001b[34m[305]#011val's binary_logloss: 0.189294#011train's binary_logloss: 0.0618449\u001b[0m\n",
      "\u001b[34m[306]#011val's binary_logloss: 0.189308#011train's binary_logloss: 0.0616025\u001b[0m\n",
      "\u001b[34m[307]#011val's binary_logloss: 0.189467#011train's binary_logloss: 0.0613631\u001b[0m\n",
      "\u001b[35m[306]#011val's binary_logloss: 0.161446#011train's binary_logloss: 0.0663253\u001b[0m\n",
      "\u001b[35m[307]#011val's binary_logloss: 0.161462#011train's binary_logloss: 0.0661129\u001b[0m\n",
      "\u001b[34m[308]#011val's binary_logloss: 0.189562#011train's binary_logloss: 0.0611415\u001b[0m\n",
      "\u001b[34m[309]#011val's binary_logloss: 0.189656#011train's binary_logloss: 0.0609284\u001b[0m\n",
      "\u001b[35m[308]#011val's binary_logloss: 0.161458#011train's binary_logloss: 0.0659137\u001b[0m\n",
      "\u001b[35m[309]#011val's binary_logloss: 0.161502#011train's binary_logloss: 0.0657267\u001b[0m\n",
      "\u001b[35m[310]#011val's binary_logloss: 0.161424#011train's binary_logloss: 0.0655315\u001b[0m\n",
      "\u001b[34m[310]#011val's binary_logloss: 0.189631#011train's binary_logloss: 0.0607356\u001b[0m\n",
      "\u001b[34m[311]#011val's binary_logloss: 0.189619#011train's binary_logloss: 0.060605\u001b[0m\n",
      "\u001b[34m[312]#011val's binary_logloss: 0.189655#011train's binary_logloss: 0.0604255\u001b[0m\n",
      "\u001b[35m[311]#011val's binary_logloss: 0.161401#011train's binary_logloss: 0.0654065\u001b[0m\n",
      "\u001b[35m[312]#011val's binary_logloss: 0.16142#011train's binary_logloss: 0.0652264\u001b[0m\n",
      "\u001b[35m[313]#011val's binary_logloss: 0.161403#011train's binary_logloss: 0.0650519\u001b[0m\n",
      "\u001b[34m[313]#011val's binary_logloss: 0.189699#011train's binary_logloss: 0.060282\u001b[0m\n",
      "\u001b[34m[314]#011val's binary_logloss: 0.189746#011train's binary_logloss: 0.0601752\u001b[0m\n",
      "\u001b[34m[315]#011val's binary_logloss: 0.189809#011train's binary_logloss: 0.0600206\u001b[0m\n",
      "\u001b[35m[314]#011val's binary_logloss: 0.16142#011train's binary_logloss: 0.0649185\u001b[0m\n",
      "\u001b[35m[315]#011val's binary_logloss: 0.161475#011train's binary_logloss: 0.0647114\u001b[0m\n",
      "\u001b[34m[316]#011val's binary_logloss: 0.189871#011train's binary_logloss: 0.0598362\u001b[0m\n",
      "\u001b[34m[317]#011val's binary_logloss: 0.189885#011train's binary_logloss: 0.0596497\u001b[0m\n",
      "\u001b[35m[316]#011val's binary_logloss: 0.161555#011train's binary_logloss: 0.0645059\u001b[0m\n",
      "\u001b[35m[317]#011val's binary_logloss: 0.161582#011train's binary_logloss: 0.0643179\u001b[0m\n",
      "\u001b[34m[318]#011val's binary_logloss: 0.189833#011train's binary_logloss: 0.0594535\u001b[0m\n",
      "\u001b[34m[319]#011val's binary_logloss: 0.189899#011train's binary_logloss: 0.0592769\u001b[0m\n",
      "\u001b[35m[318]#011val's binary_logloss: 0.161633#011train's binary_logloss: 0.0641035\u001b[0m\n",
      "\u001b[35m[319]#011val's binary_logloss: 0.161737#011train's binary_logloss: 0.0639288\u001b[0m\n",
      "\u001b[35m[320]#011val's binary_logloss: 0.161782#011train's binary_logloss: 0.0637448\u001b[0m\n",
      "\u001b[34m[320]#011val's binary_logloss: 0.189928#011train's binary_logloss: 0.0591152\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[321]#011val's binary_logloss: 0.189951#011train's binary_logloss: 0.0590398\u001b[0m\n",
      "\u001b[34m[322]#011val's binary_logloss: 0.189918#011train's binary_logloss: 0.0589034\u001b[0m\n",
      "\u001b[34m[323]#011val's binary_logloss: 0.189998#011train's binary_logloss: 0.0587227\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[321]#011val's binary_logloss: 0.161779#011train's binary_logloss: 0.0636607\u001b[0m\n",
      "\u001b[35m[322]#011val's binary_logloss: 0.161815#011train's binary_logloss: 0.063489\u001b[0m\n",
      "\u001b[35m[323]#011val's binary_logloss: 0.161837#011train's binary_logloss: 0.0633181\u001b[0m\n",
      "\u001b[34m[324]#011val's binary_logloss: 0.190014#011train's binary_logloss: 0.058563\u001b[0m\n",
      "\u001b[34m[325]#011val's binary_logloss: 0.18996#011train's binary_logloss: 0.0584297\u001b[0m\n",
      "\u001b[35m[324]#011val's binary_logloss: 0.161799#011train's binary_logloss: 0.0631724\u001b[0m\n",
      "\u001b[35m[325]#011val's binary_logloss: 0.161815#011train's binary_logloss: 0.063005\u001b[0m\n",
      "\u001b[34m[326]#011val's binary_logloss: 0.189993#011train's binary_logloss: 0.058265\u001b[0m\n",
      "\u001b[34m[327]#011val's binary_logloss: 0.189975#011train's binary_logloss: 0.0581369\u001b[0m\n",
      "\u001b[34m[328]#011val's binary_logloss: 0.190023#011train's binary_logloss: 0.0579597\u001b[0m\n",
      "\u001b[35m[326]#011val's binary_logloss: 0.16185#011train's binary_logloss: 0.0628297\u001b[0m\n",
      "\u001b[35m[327]#011val's binary_logloss: 0.161839#011train's binary_logloss: 0.0626861\u001b[0m\n",
      "\u001b[35m[328]#011val's binary_logloss: 0.161907#011train's binary_logloss: 0.0625047\u001b[0m\n",
      "\u001b[34m[329]#011val's binary_logloss: 0.190128#011train's binary_logloss: 0.057785\u001b[0m\n",
      "\u001b[34m[330]#011val's binary_logloss: 0.190104#011train's binary_logloss: 0.0576185\u001b[0m\n",
      "\u001b[35m[329]#011val's binary_logloss: 0.161935#011train's binary_logloss: 0.0623474\u001b[0m\n",
      "\u001b[35m[330]#011val's binary_logloss: 0.161969#011train's binary_logloss: 0.062193\u001b[0m\n",
      "\u001b[35m[331]#011val's binary_logloss: 0.162023#011train's binary_logloss: 0.0619944\u001b[0m\n",
      "\u001b[34m[331]#011val's binary_logloss: 0.190004#011train's binary_logloss: 0.057453\u001b[0m\n",
      "\u001b[34m[332]#011val's binary_logloss: 0.190031#011train's binary_logloss: 0.0572703\u001b[0m\n",
      "\u001b[34m[333]#011val's binary_logloss: 0.190092#011train's binary_logloss: 0.0570948\u001b[0m\n",
      "\u001b[35m[332]#011val's binary_logloss: 0.162067#011train's binary_logloss: 0.0618004\u001b[0m\n",
      "\u001b[35m[333]#011val's binary_logloss: 0.162149#011train's binary_logloss: 0.0616298\u001b[0m\n",
      "\u001b[34m[334]#011val's binary_logloss: 0.190087#011train's binary_logloss: 0.0569191\u001b[0m\n",
      "\u001b[35m[334]#011val's binary_logloss: 0.162241#011train's binary_logloss: 0.0614358\u001b[0m\n",
      "\u001b[34m[335]#011val's binary_logloss: 0.190069#011train's binary_logloss: 0.0567476\u001b[0m\n",
      "\u001b[35m[335]#011val's binary_logloss: 0.162268#011train's binary_logloss: 0.0612383\u001b[0m\n",
      "\u001b[34m[336]#011val's binary_logloss: 0.190013#011train's binary_logloss: 0.0566111\u001b[0m\n",
      "\u001b[34m[337]#011val's binary_logloss: 0.19011#011train's binary_logloss: 0.056449\u001b[0m\n",
      "\u001b[34m[338]#011val's binary_logloss: 0.190091#011train's binary_logloss: 0.0563115\u001b[0m\n",
      "\u001b[35m[336]#011val's binary_logloss: 0.162271#011train's binary_logloss: 0.0610697\u001b[0m\n",
      "\u001b[35m[337]#011val's binary_logloss: 0.162287#011train's binary_logloss: 0.0608896\u001b[0m\n",
      "\u001b[35m[338]#011val's binary_logloss: 0.162308#011train's binary_logloss: 0.060722\u001b[0m\n",
      "\u001b[34m[339]#011val's binary_logloss: 0.190208#011train's binary_logloss: 0.0561568\u001b[0m\n",
      "\u001b[35m[339]#011val's binary_logloss: 0.162306#011train's binary_logloss: 0.0605667\u001b[0m\n",
      "\u001b[35m[340]#011val's binary_logloss: 0.162294#011train's binary_logloss: 0.0603937\u001b[0m\n",
      "\u001b[34m[340]#011val's binary_logloss: 0.190314#011train's binary_logloss: 0.0559809\u001b[0m\n",
      "\u001b[34m[341]#011val's binary_logloss: 0.190433#011train's binary_logloss: 0.0558358\u001b[0m\n",
      "\u001b[34m[342]#011val's binary_logloss: 0.190509#011train's binary_logloss: 0.0556655\u001b[0m\n",
      "\u001b[34m[343]#011val's binary_logloss: 0.190566#011train's binary_logloss: 0.0555258\u001b[0m\n",
      "\u001b[35m[341]#011val's binary_logloss: 0.162362#011train's binary_logloss: 0.0602494\u001b[0m\n",
      "\u001b[35m[342]#011val's binary_logloss: 0.162354#011train's binary_logloss: 0.0600747\u001b[0m\n",
      "\u001b[35m[343]#011val's binary_logloss: 0.162408#011train's binary_logloss: 0.0599073\u001b[0m\n",
      "\u001b[34m[344]#011val's binary_logloss: 0.190622#011train's binary_logloss: 0.0553983\u001b[0m\n",
      "\u001b[34m[345]#011val's binary_logloss: 0.190723#011train's binary_logloss: 0.0552708\u001b[0m\n",
      "\u001b[34m[346]#011val's binary_logloss: 0.190778#011train's binary_logloss: 0.055118\u001b[0m\n",
      "\u001b[35m[344]#011val's binary_logloss: 0.162465#011train's binary_logloss: 0.0597728\u001b[0m\n",
      "\u001b[35m[345]#011val's binary_logloss: 0.162532#011train's binary_logloss: 0.0596209\u001b[0m\n",
      "\u001b[35m[346]#011val's binary_logloss: 0.162551#011train's binary_logloss: 0.0594502\u001b[0m\n",
      "\u001b[35m[347]#011val's binary_logloss: 0.162593#011train's binary_logloss: 0.0593096\u001b[0m\n",
      "\u001b[34m[347]#011val's binary_logloss: 0.190847#011train's binary_logloss: 0.0549911\u001b[0m\n",
      "\u001b[34m[348]#011val's binary_logloss: 0.190817#011train's binary_logloss: 0.05483\u001b[0m\n",
      "\u001b[35m[348]#011val's binary_logloss: 0.162672#011train's binary_logloss: 0.0591467\u001b[0m\n",
      "\u001b[35m[349]#011val's binary_logloss: 0.162686#011train's binary_logloss: 0.0589744\u001b[0m\n",
      "\u001b[34m[349]#011val's binary_logloss: 0.190808#011train's binary_logloss: 0.054697\u001b[0m\n",
      "\u001b[34m[350]#011val's binary_logloss: 0.190803#011train's binary_logloss: 0.0545705\u001b[0m\n",
      "\u001b[34m[351]#011val's binary_logloss: 0.190902#011train's binary_logloss: 0.0543939\u001b[0m\n",
      "\u001b[35m[350]#011val's binary_logloss: 0.162703#011train's binary_logloss: 0.0588567\u001b[0m\n",
      "\u001b[35m[351]#011val's binary_logloss: 0.162734#011train's binary_logloss: 0.058663\u001b[0m\n",
      "\u001b[34m[352]#011val's binary_logloss: 0.190984#011train's binary_logloss: 0.0542155\u001b[0m\n",
      "\u001b[34m[353]#011val's binary_logloss: 0.191049#011train's binary_logloss: 0.0540454\u001b[0m\n",
      "\u001b[35m[352]#011val's binary_logloss: 0.162839#011train's binary_logloss: 0.0584759\u001b[0m\n",
      "\u001b[35m[353]#011val's binary_logloss: 0.162889#011train's binary_logloss: 0.058283\u001b[0m\n",
      "\u001b[34m[354]#011val's binary_logloss: 0.191246#011train's binary_logloss: 0.0538594\u001b[0m\n",
      "\u001b[34m[355]#011val's binary_logloss: 0.191262#011train's binary_logloss: 0.0537605\u001b[0m\n",
      "\u001b[35m[354]#011val's binary_logloss: 0.162971#011train's binary_logloss: 0.0581067\u001b[0m\n",
      "\u001b[35m[355]#011val's binary_logloss: 0.162984#011train's binary_logloss: 0.0579822\u001b[0m\n",
      "\u001b[34m[356]#011val's binary_logloss: 0.191326#011train's binary_logloss: 0.0536457\u001b[0m\n",
      "\u001b[34m[357]#011val's binary_logloss: 0.191296#011train's binary_logloss: 0.0535615\u001b[0m\n",
      "\u001b[34m[358]#011val's binary_logloss: 0.191277#011train's binary_logloss: 0.0534283\u001b[0m\n",
      "\u001b[35m[356]#011val's binary_logloss: 0.162972#011train's binary_logloss: 0.0578597\u001b[0m\n",
      "\u001b[35m[357]#011val's binary_logloss: 0.162954#011train's binary_logloss: 0.0577258\u001b[0m\n",
      "\u001b[35m[358]#011val's binary_logloss: 0.162973#011train's binary_logloss: 0.0575687\u001b[0m\n",
      "\u001b[35m[359]#011val's binary_logloss: 0.163015#011train's binary_logloss: 0.0573982\u001b[0m\n",
      "\u001b[34m[359]#011val's binary_logloss: 0.191372#011train's binary_logloss: 0.0532779\u001b[0m\n",
      "\u001b[34m[360]#011val's binary_logloss: 0.191416#011train's binary_logloss: 0.0531544\u001b[0m\n",
      "\u001b[34m[361]#011val's binary_logloss: 0.191481#011train's binary_logloss: 0.0530118\u001b[0m\n",
      "\u001b[35m[360]#011val's binary_logloss: 0.163024#011train's binary_logloss: 0.0572482\u001b[0m\n",
      "\u001b[35m[361]#011val's binary_logloss: 0.163046#011train's binary_logloss: 0.0571094\u001b[0m\n",
      "\u001b[35m[362]#011val's binary_logloss: 0.163109#011train's binary_logloss: 0.056937\u001b[0m\n",
      "\u001b[34m[362]#011val's binary_logloss: 0.191499#011train's binary_logloss: 0.0528649\u001b[0m\n",
      "\u001b[34m[363]#011val's binary_logloss: 0.191606#011train's binary_logloss: 0.0526943\u001b[0m\n",
      "\u001b[34m[364]#011val's binary_logloss: 0.191593#011train's binary_logloss: 0.0525383\u001b[0m\n",
      "\u001b[35m[363]#011val's binary_logloss: 0.163141#011train's binary_logloss: 0.0567687\u001b[0m\n",
      "\u001b[35m[364]#011val's binary_logloss: 0.163206#011train's binary_logloss: 0.0565926\u001b[0m\n",
      "\u001b[34m[365]#011val's binary_logloss: 0.191611#011train's binary_logloss: 0.0523765\u001b[0m\n",
      "\u001b[34m[366]#011val's binary_logloss: 0.191591#011train's binary_logloss: 0.0521881\u001b[0m\n",
      "\u001b[35m[365]#011val's binary_logloss: 0.163257#011train's binary_logloss: 0.0564275\u001b[0m\n",
      "\u001b[35m[366]#011val's binary_logloss: 0.1633#011train's binary_logloss: 0.0562565\u001b[0m\n",
      "\u001b[35m[367]#011val's binary_logloss: 0.163335#011train's binary_logloss: 0.0561139\u001b[0m\n",
      "\u001b[34m[367]#011val's binary_logloss: 0.191653#011train's binary_logloss: 0.0520433\u001b[0m\n",
      "\u001b[34m[368]#011val's binary_logloss: 0.191649#011train's binary_logloss: 0.0518573\u001b[0m\n",
      "\u001b[34m[369]#011val's binary_logloss: 0.191653#011train's binary_logloss: 0.0516952\u001b[0m\n",
      "\u001b[35m[368]#011val's binary_logloss: 0.163389#011train's binary_logloss: 0.0559386\u001b[0m\n",
      "\u001b[35m[369]#011val's binary_logloss: 0.163439#011train's binary_logloss: 0.0557538\u001b[0m\n",
      "\u001b[35m[370]#011val's binary_logloss: 0.16342#011train's binary_logloss: 0.0555914\u001b[0m\n",
      "\u001b[34m[370]#011val's binary_logloss: 0.191688#011train's binary_logloss: 0.0515286\u001b[0m\n",
      "\u001b[34m[371]#011val's binary_logloss: 0.191727#011train's binary_logloss: 0.0514257\u001b[0m\n",
      "\u001b[34m[372]#011val's binary_logloss: 0.191631#011train's binary_logloss: 0.0513267\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[373]#011val's binary_logloss: 0.191694#011train's binary_logloss: 0.0512642\u001b[0m\n",
      "\u001b[35m[371]#011val's binary_logloss: 0.16342#011train's binary_logloss: 0.0554766\u001b[0m\n",
      "\u001b[35m[372]#011val's binary_logloss: 0.163433#011train's binary_logloss: 0.0553714\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[373]#011val's binary_logloss: 0.163408#011train's binary_logloss: 0.0552798\u001b[0m\n",
      "\u001b[34m[374]#011val's binary_logloss: 0.191668#011train's binary_logloss: 0.0511772\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[375]#011val's binary_logloss: 0.191577#011train's binary_logloss: 0.0511179\u001b[0m\n",
      "\u001b[34m[376]#011val's binary_logloss: 0.191723#011train's binary_logloss: 0.0509902\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[377]#011val's binary_logloss: 0.19175#011train's binary_logloss: 0.050932\u001b[0m\n",
      "\u001b[35m[374]#011val's binary_logloss: 0.163456#011train's binary_logloss: 0.0551687\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[375]#011val's binary_logloss: 0.163461#011train's binary_logloss: 0.0550899\u001b[0m\n",
      "\u001b[35m[376]#011val's binary_logloss: 0.163469#011train's binary_logloss: 0.0549544\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[377]#011val's binary_logloss: 0.163472#011train's binary_logloss: 0.0548832\u001b[0m\n",
      "\u001b[34m[378]#011val's binary_logloss: 0.191802#011train's binary_logloss: 0.050841\u001b[0m\n",
      "\u001b[34m[379]#011val's binary_logloss: 0.191862#011train's binary_logloss: 0.0507415\u001b[0m\n",
      "\u001b[35m[378]#011val's binary_logloss: 0.16345#011train's binary_logloss: 0.0547512\u001b[0m\n",
      "\u001b[35m[379]#011val's binary_logloss: 0.163486#011train's binary_logloss: 0.0546299\u001b[0m\n",
      "\u001b[34m[380]#011val's binary_logloss: 0.191947#011train's binary_logloss: 0.0506393\u001b[0m\n",
      "\u001b[34m[381]#011val's binary_logloss: 0.191971#011train's binary_logloss: 0.0504855\u001b[0m\n",
      "\u001b[34m[382]#011val's binary_logloss: 0.191972#011train's binary_logloss: 0.0503654\u001b[0m\n",
      "\u001b[35m[380]#011val's binary_logloss: 0.163527#011train's binary_logloss: 0.0545067\u001b[0m\n",
      "\u001b[35m[381]#011val's binary_logloss: 0.163509#011train's binary_logloss: 0.0543806\u001b[0m\n",
      "\u001b[35m[382]#011val's binary_logloss: 0.163495#011train's binary_logloss: 0.0542786\u001b[0m\n",
      "\u001b[35m[383]#011val's binary_logloss: 0.163519#011train's binary_logloss: 0.0541608\u001b[0m\n",
      "\u001b[34m[383]#011val's binary_logloss: 0.191945#011train's binary_logloss: 0.0502312\u001b[0m\n",
      "\u001b[34m[384]#011val's binary_logloss: 0.191967#011train's binary_logloss: 0.0500904\u001b[0m\n",
      "\u001b[34m[385]#011val's binary_logloss: 0.19206#011train's binary_logloss: 0.0499818\u001b[0m\n",
      "\u001b[34m[386]#011val's binary_logloss: 0.192119#011train's binary_logloss: 0.0498927\u001b[0m\n",
      "\u001b[35m[384]#011val's binary_logloss: 0.163563#011train's binary_logloss: 0.0540373\u001b[0m\n",
      "\u001b[35m[385]#011val's binary_logloss: 0.163621#011train's binary_logloss: 0.0539416\u001b[0m\n",
      "\u001b[35m[386]#011val's binary_logloss: 0.16365#011train's binary_logloss: 0.0538563\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[387]#011val's binary_logloss: 0.192139#011train's binary_logloss: 0.0497895\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[388]#011val's binary_logloss: 0.192099#011train's binary_logloss: 0.0496911\u001b[0m\n",
      "\u001b[34m[389]#011val's binary_logloss: 0.192155#011train's binary_logloss: 0.0495899\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[387]#011val's binary_logloss: 0.163679#011train's binary_logloss: 0.053746\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[388]#011val's binary_logloss: 0.163711#011train's binary_logloss: 0.0536256\u001b[0m\n",
      "\u001b[35m[389]#011val's binary_logloss: 0.163743#011train's binary_logloss: 0.0535265\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning]\u001b[0m\n",
      "\u001b[35mNo further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[390]#011val's binary_logloss: 0.163743#011train's binary_logloss: 0.0534445\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[390]#011val's binary_logloss: 0.192209#011train's binary_logloss: 0.0495067\u001b[0m\n",
      "\u001b[34m[391]#011val's binary_logloss: 0.192206#011train's binary_logloss: 0.0493668\u001b[0m\n",
      "\u001b[34m[392]#011val's binary_logloss: 0.192237#011train's binary_logloss: 0.0492494\u001b[0m\n",
      "\u001b[34m[393]#011val's binary_logloss: 0.192311#011train's binary_logloss: 0.0491493\u001b[0m\n",
      "\u001b[35m[391]#011val's binary_logloss: 0.163744#011train's binary_logloss: 0.0532842\u001b[0m\n",
      "\u001b[35m[392]#011val's binary_logloss: 0.163759#011train's binary_logloss: 0.0531672\u001b[0m\n",
      "\u001b[35m[393]#011val's binary_logloss: 0.163801#011train's binary_logloss: 0.0530263\u001b[0m\n",
      "\u001b[34m[394]#011val's binary_logloss: 0.192357#011train's binary_logloss: 0.0490569\u001b[0m\n",
      "\u001b[34m[395]#011val's binary_logloss: 0.192374#011train's binary_logloss: 0.0489442\u001b[0m\n",
      "\u001b[34m[396]#011val's binary_logloss: 0.192511#011train's binary_logloss: 0.0488161\u001b[0m\n",
      "\u001b[35m[394]#011val's binary_logloss: 0.163829#011train's binary_logloss: 0.0529153\u001b[0m\n",
      "\u001b[35m[395]#011val's binary_logloss: 0.163855#011train's binary_logloss: 0.052807\u001b[0m\n",
      "\u001b[35m[396]#011val's binary_logloss: 0.163915#011train's binary_logloss: 0.0526831\u001b[0m\n",
      "\u001b[34m[397]#011val's binary_logloss: 0.192531#011train's binary_logloss: 0.0487178\u001b[0m\n",
      "\u001b[34m[398]#011val's binary_logloss: 0.192632#011train's binary_logloss: 0.0485877\u001b[0m\n",
      "\u001b[35m[397]#011val's binary_logloss: 0.163956#011train's binary_logloss: 0.0525804\u001b[0m\n",
      "\u001b[35m[398]#011val's binary_logloss: 0.164011#011train's binary_logloss: 0.0524302\u001b[0m\n",
      "\u001b[34m[399]#011val's binary_logloss: 0.19275#011train's binary_logloss: 0.0484649\u001b[0m\n",
      "\u001b[34m[400]#011val's binary_logloss: 0.192775#011train's binary_logloss: 0.0483821\u001b[0m\n",
      "\u001b[34m[401]#011val's binary_logloss: 0.192718#011train's binary_logloss: 0.0482511\u001b[0m\n",
      "\u001b[35m[399]#011val's binary_logloss: 0.164125#011train's binary_logloss: 0.0522898\u001b[0m\n",
      "\u001b[35m[400]#011val's binary_logloss: 0.164184#011train's binary_logloss: 0.0522096\u001b[0m\n",
      "\u001b[35m[401]#011val's binary_logloss: 0.164198#011train's binary_logloss: 0.0520766\u001b[0m\n",
      "\u001b[34m[402]#011val's binary_logloss: 0.19272#011train's binary_logloss: 0.0480936\u001b[0m\n",
      "\u001b[34m[403]#011val's binary_logloss: 0.192847#011train's binary_logloss: 0.0479562\u001b[0m\n",
      "\u001b[35m[402]#011val's binary_logloss: 0.164211#011train's binary_logloss: 0.0519152\u001b[0m\n",
      "\u001b[35m[403]#011val's binary_logloss: 0.164285#011train's binary_logloss: 0.0517626\u001b[0m\n",
      "\u001b[34m[404]#011val's binary_logloss: 0.192913#011train's binary_logloss: 0.0478203\u001b[0m\n",
      "\u001b[34m[405]#011val's binary_logloss: 0.192995#011train's binary_logloss: 0.0476904\u001b[0m\n",
      "\u001b[35m[404]#011val's binary_logloss: 0.164391#011train's binary_logloss: 0.0515938\u001b[0m\n",
      "\u001b[35m[405]#011val's binary_logloss: 0.164471#011train's binary_logloss: 0.0514394\u001b[0m\n",
      "\u001b[35m[406]#011val's binary_logloss: 0.164485#011train's binary_logloss: 0.0513622\u001b[0m\n",
      "\u001b[34m[406]#011val's binary_logloss: 0.192947#011train's binary_logloss: 0.0476257\u001b[0m\n",
      "\u001b[34m[407]#011val's binary_logloss: 0.192955#011train's binary_logloss: 0.0475229\u001b[0m\n",
      "\u001b[34m[408]#011val's binary_logloss: 0.192911#011train's binary_logloss: 0.0474546\u001b[0m\n",
      "\u001b[34m[409]#011val's binary_logloss: 0.192994#011train's binary_logloss: 0.0473366\u001b[0m\n",
      "\u001b[35m[407]#011val's binary_logloss: 0.164463#011train's binary_logloss: 0.0512609\u001b[0m\n",
      "\u001b[35m[408]#011val's binary_logloss: 0.164437#011train's binary_logloss: 0.0511955\u001b[0m\n",
      "\u001b[35m[409]#011val's binary_logloss: 0.164498#011train's binary_logloss: 0.0510851\u001b[0m\n",
      "\u001b[34m[410]#011val's binary_logloss: 0.193032#011train's binary_logloss: 0.0472252\u001b[0m\n",
      "\u001b[34m[411]#011val's binary_logloss: 0.193108#011train's binary_logloss: 0.0471131\u001b[0m\n",
      "\u001b[34m[412]#011val's binary_logloss: 0.193195#011train's binary_logloss: 0.0470298\u001b[0m\n",
      "\u001b[35m[410]#011val's binary_logloss: 0.164511#011train's binary_logloss: 0.0509468\u001b[0m\n",
      "\u001b[35m[411]#011val's binary_logloss: 0.164524#011train's binary_logloss: 0.0508215\u001b[0m\n",
      "\u001b[35m[412]#011val's binary_logloss: 0.164569#011train's binary_logloss: 0.0507117\u001b[0m\n",
      "\u001b[34m[413]#011val's binary_logloss: 0.193257#011train's binary_logloss: 0.0469023\u001b[0m\n",
      "\u001b[34m[414]#011val's binary_logloss: 0.19339#011train's binary_logloss: 0.0467892\u001b[0m\n",
      "\u001b[34m[415]#011val's binary_logloss: 0.193492#011train's binary_logloss: 0.0466871\u001b[0m\n",
      "\u001b[35m[413]#011val's binary_logloss: 0.164617#011train's binary_logloss: 0.0505829\u001b[0m\n",
      "\u001b[35m[414]#011val's binary_logloss: 0.164652#011train's binary_logloss: 0.0504584\u001b[0m\n",
      "\u001b[35m[415]#011val's binary_logloss: 0.164663#011train's binary_logloss: 0.0503489\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[416]#011val's binary_logloss: 0.193503#011train's binary_logloss: 0.0466026\u001b[0m\n",
      "\u001b[34m[417]#011val's binary_logloss: 0.193484#011train's binary_logloss: 0.0464757\u001b[0m\n",
      "\u001b[34m[418]#011val's binary_logloss: 0.193556#011train's binary_logloss: 0.0463592\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[416]#011val's binary_logloss: 0.164642#011train's binary_logloss: 0.0502589\u001b[0m\n",
      "\u001b[35m[417]#011val's binary_logloss: 0.16465#011train's binary_logloss: 0.050104\u001b[0m\n",
      "\u001b[35m[418]#011val's binary_logloss: 0.164684#011train's binary_logloss: 0.0499471\u001b[0m\n",
      "\u001b[34m[419]#011val's binary_logloss: 0.193582#011train's binary_logloss: 0.0462289\u001b[0m\n",
      "\u001b[34m[420]#011val's binary_logloss: 0.19359#011train's binary_logloss: 0.0461511\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[421]#011val's binary_logloss: 0.193586#011train's binary_logloss: 0.0460634\u001b[0m\n",
      "\u001b[34m[422]#011val's binary_logloss: 0.193652#011train's binary_logloss: 0.0459508\u001b[0m\n",
      "\u001b[35m[419]#011val's binary_logloss: 0.164729#011train's binary_logloss: 0.0498162\u001b[0m\n",
      "\u001b[35m[420]#011val's binary_logloss: 0.164791#011train's binary_logloss: 0.049713\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[421]#011val's binary_logloss: 0.164818#011train's binary_logloss: 0.0496066\u001b[0m\n",
      "\u001b[35m[422]#011val's binary_logloss: 0.164899#011train's binary_logloss: 0.0494772\u001b[0m\n",
      "\u001b[34m[423]#011val's binary_logloss: 0.193736#011train's binary_logloss: 0.0458591\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[424]#011val's binary_logloss: 0.193815#011train's binary_logloss: 0.0458002\u001b[0m\n",
      "\u001b[34m[425]#011val's binary_logloss: 0.193821#011train's binary_logloss: 0.0457282\u001b[0m\n",
      "\u001b[35m[423]#011val's binary_logloss: 0.164908#011train's binary_logloss: 0.0493786\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[424]#011val's binary_logloss: 0.164889#011train's binary_logloss: 0.0493091\u001b[0m\n",
      "\u001b[35m[425]#011val's binary_logloss: 0.164919#011train's binary_logloss: 0.0492164\u001b[0m\n",
      "\u001b[35m[426]#011val's binary_logloss: 0.164961#011train's binary_logloss: 0.0490878\u001b[0m\n",
      "\u001b[34m[426]#011val's binary_logloss: 0.193928#011train's binary_logloss: 0.0456241\u001b[0m\n",
      "\u001b[34m[427]#011val's binary_logloss: 0.194011#011train's binary_logloss: 0.0455098\u001b[0m\n",
      "\u001b[34m[428]#011val's binary_logloss: 0.194039#011train's binary_logloss: 0.0453734\u001b[0m\n",
      "\u001b[35m[427]#011val's binary_logloss: 0.165038#011train's binary_logloss: 0.0489478\u001b[0m\n",
      "\u001b[35m[428]#011val's binary_logloss: 0.165058#011train's binary_logloss: 0.0488142\u001b[0m\n",
      "\u001b[34m[429]#011val's binary_logloss: 0.193993#011train's binary_logloss: 0.0452903\u001b[0m\n",
      "\u001b[34m[430]#011val's binary_logloss: 0.194055#011train's binary_logloss: 0.045188\u001b[0m\n",
      "\u001b[35m[429]#011val's binary_logloss: 0.165078#011train's binary_logloss: 0.0487066\u001b[0m\n",
      "\u001b[35m[430]#011val's binary_logloss: 0.165129#011train's binary_logloss: 0.048583\u001b[0m\n",
      "\u001b[35m[431]#011val's binary_logloss: 0.165094#011train's binary_logloss: 0.0484939\u001b[0m\n",
      "\u001b[34m[431]#011val's binary_logloss: 0.194141#011train's binary_logloss: 0.0450959\u001b[0m\n",
      "\u001b[34m[432]#011val's binary_logloss: 0.194121#011train's binary_logloss: 0.04499\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[433]#011val's binary_logloss: 0.194228#011train's binary_logloss: 0.0449184\u001b[0m\n",
      "\u001b[34m[434]#011val's binary_logloss: 0.194262#011train's binary_logloss: 0.0448303\u001b[0m\n",
      "\u001b[35m[432]#011val's binary_logloss: 0.16506#011train's binary_logloss: 0.0483755\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[433]#011val's binary_logloss: 0.165102#011train's binary_logloss: 0.0483042\u001b[0m\n",
      "\u001b[35m[434]#011val's binary_logloss: 0.165135#011train's binary_logloss: 0.0482031\u001b[0m\n",
      "\u001b[34m[435]#011val's binary_logloss: 0.194353#011train's binary_logloss: 0.0447187\u001b[0m\n",
      "\u001b[34m[436]#011val's binary_logloss: 0.194346#011train's binary_logloss: 0.044591\u001b[0m\n",
      "\u001b[34m[437]#011val's binary_logloss: 0.194378#011train's binary_logloss: 0.0444853\u001b[0m\n",
      "\u001b[35m[435]#011val's binary_logloss: 0.165117#011train's binary_logloss: 0.0480968\u001b[0m\n",
      "\u001b[35m[436]#011val's binary_logloss: 0.165129#011train's binary_logloss: 0.0479671\u001b[0m\n",
      "\u001b[35m[437]#011val's binary_logloss: 0.16517#011train's binary_logloss: 0.0478558\u001b[0m\n",
      "\u001b[34m[438]#011val's binary_logloss: 0.194454#011train's binary_logloss: 0.044366\u001b[0m\n",
      "\u001b[34m[439]#011val's binary_logloss: 0.194574#011train's binary_logloss: 0.0442571\u001b[0m\n",
      "\u001b[35m[438]#011val's binary_logloss: 0.165186#011train's binary_logloss: 0.0477305\u001b[0m\n",
      "\u001b[35m[439]#011val's binary_logloss: 0.165104#011train's binary_logloss: 0.0476134\u001b[0m\n",
      "\u001b[35m[440]#011val's binary_logloss: 0.165142#011train's binary_logloss: 0.0474868\u001b[0m\n",
      "\u001b[34m[440]#011val's binary_logloss: 0.194643#011train's binary_logloss: 0.0441465\u001b[0m\n",
      "\u001b[34m[441]#011val's binary_logloss: 0.194784#011train's binary_logloss: 0.0440249\u001b[0m\n",
      "\u001b[34m[442]#011val's binary_logloss: 0.19488#011train's binary_logloss: 0.0438859\u001b[0m\n",
      "\u001b[35m[441]#011val's binary_logloss: 0.165169#011train's binary_logloss: 0.0473398\u001b[0m\n",
      "\u001b[35m[442]#011val's binary_logloss: 0.165179#011train's binary_logloss: 0.0472025\u001b[0m\n",
      "\u001b[35m[443]#011val's binary_logloss: 0.165245#011train's binary_logloss: 0.0470542\u001b[0m\n",
      "\u001b[34m[443]#011val's binary_logloss: 0.194941#011train's binary_logloss: 0.0437468\u001b[0m\n",
      "\u001b[34m[444]#011val's binary_logloss: 0.194976#011train's binary_logloss: 0.043621\u001b[0m\n",
      "\u001b[34m[445]#011val's binary_logloss: 0.195018#011train's binary_logloss: 0.043497\u001b[0m\n",
      "\u001b[35m[444]#011val's binary_logloss: 0.165308#011train's binary_logloss: 0.0469075\u001b[0m\n",
      "\u001b[35m[445]#011val's binary_logloss: 0.16533#011train's binary_logloss: 0.0467657\u001b[0m\n",
      "\u001b[34m[446]#011val's binary_logloss: 0.195056#011train's binary_logloss: 0.0434116\u001b[0m\n",
      "\u001b[34m[447]#011val's binary_logloss: 0.195132#011train's binary_logloss: 0.0433257\u001b[0m\n",
      "\u001b[34m[448]#011val's binary_logloss: 0.195211#011train's binary_logloss: 0.0432319\u001b[0m\n",
      "\u001b[35m[446]#011val's binary_logloss: 0.165373#011train's binary_logloss: 0.0466667\u001b[0m\n",
      "\u001b[35m[447]#011val's binary_logloss: 0.165438#011train's binary_logloss: 0.0465691\u001b[0m\n",
      "\u001b[35m[448]#011val's binary_logloss: 0.165533#011train's binary_logloss: 0.0464674\u001b[0m\n",
      "\u001b[34m[449]#011val's binary_logloss: 0.195193#011train's binary_logloss: 0.0431128\u001b[0m\n",
      "\u001b[34m[450]#011val's binary_logloss: 0.195182#011train's binary_logloss: 0.043035\u001b[0m\n",
      "\u001b[35m[449]#011val's binary_logloss: 0.165547#011train's binary_logloss: 0.0463296\u001b[0m\n",
      "\u001b[35m[450]#011val's binary_logloss: 0.165594#011train's binary_logloss: 0.0462508\u001b[0m\n",
      "\u001b[35m[451]#011val's binary_logloss: 0.165667#011train's binary_logloss: 0.0461322\u001b[0m\n",
      "\u001b[34m[451]#011val's binary_logloss: 0.195263#011train's binary_logloss: 0.0429252\u001b[0m\n",
      "\u001b[34m[452]#011val's binary_logloss: 0.195348#011train's binary_logloss: 0.0428189\u001b[0m\n",
      "\u001b[34m[453]#011val's binary_logloss: 0.195435#011train's binary_logloss: 0.0426931\u001b[0m\n",
      "\u001b[34m[454]#011val's binary_logloss: 0.195476#011train's binary_logloss: 0.0425765\u001b[0m\n",
      "\u001b[35m[452]#011val's binary_logloss: 0.165721#011train's binary_logloss: 0.0460245\u001b[0m\n",
      "\u001b[35m[453]#011val's binary_logloss: 0.165814#011train's binary_logloss: 0.0458989\u001b[0m\n",
      "\u001b[35m[454]#011val's binary_logloss: 0.165929#011train's binary_logloss: 0.0457781\u001b[0m\n",
      "\u001b[34m[455]#011val's binary_logloss: 0.195428#011train's binary_logloss: 0.0424784\u001b[0m\n",
      "\u001b[34m[456]#011val's binary_logloss: 0.195454#011train's binary_logloss: 0.0423753\u001b[0m\n",
      "\u001b[35m[455]#011val's binary_logloss: 0.165995#011train's binary_logloss: 0.0456905\u001b[0m\n",
      "\u001b[35m[456]#011val's binary_logloss: 0.166026#011train's binary_logloss: 0.0455886\u001b[0m\n",
      "\u001b[35m[457]#011val's binary_logloss: 0.166026#011train's binary_logloss: 0.0454804\u001b[0m\n",
      "\u001b[34m[457]#011val's binary_logloss: 0.195482#011train's binary_logloss: 0.042276\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[458]#011val's binary_logloss: 0.195527#011train's binary_logloss: 0.0421972\u001b[0m\n",
      "\u001b[34m[459]#011val's binary_logloss: 0.195638#011train's binary_logloss: 0.0420711\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[458]#011val's binary_logloss: 0.166047#011train's binary_logloss: 0.0454116\u001b[0m\n",
      "\u001b[35m[459]#011val's binary_logloss: 0.1661#011train's binary_logloss: 0.0452894\u001b[0m\n",
      "\u001b[35m[460]#011val's binary_logloss: 0.166187#011train's binary_logloss: 0.0451799\u001b[0m\n",
      "\u001b[34m[460]#011val's binary_logloss: 0.19574#011train's binary_logloss: 0.0419657\u001b[0m\n",
      "\u001b[34m[461]#011val's binary_logloss: 0.195887#011train's binary_logloss: 0.0418792\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[462]#011val's binary_logloss: 0.195939#011train's binary_logloss: 0.0418115\u001b[0m\n",
      "\u001b[35m[461]#011val's binary_logloss: 0.166208#011train's binary_logloss: 0.0450637\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[462]#011val's binary_logloss: 0.166242#011train's binary_logloss: 0.0449899\u001b[0m\n",
      "\u001b[35m[463]#011val's binary_logloss: 0.166275#011train's binary_logloss: 0.0449105\u001b[0m\n",
      "\u001b[34m[463]#011val's binary_logloss: 0.196074#011train's binary_logloss: 0.0417267\u001b[0m\n",
      "\u001b[34m[464]#011val's binary_logloss: 0.196146#011train's binary_logloss: 0.041664\u001b[0m\n",
      "\u001b[35m[464]#011val's binary_logloss: 0.1663#011train's binary_logloss: 0.0448126\u001b[0m\n",
      "\u001b[35m[465]#011val's binary_logloss: 0.166387#011train's binary_logloss: 0.0447346\u001b[0m\n",
      "\u001b[34m[465]#011val's binary_logloss: 0.196232#011train's binary_logloss: 0.0415989\u001b[0m\n",
      "\u001b[34m[466]#011val's binary_logloss: 0.196236#011train's binary_logloss: 0.0415067\u001b[0m\n",
      "\u001b[34m[467]#011val's binary_logloss: 0.196321#011train's binary_logloss: 0.0414176\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[468]#011val's binary_logloss: 0.196308#011train's binary_logloss: 0.041368\u001b[0m\n",
      "\u001b[35m[466]#011val's binary_logloss: 0.166468#011train's binary_logloss: 0.0446256\u001b[0m\n",
      "\u001b[35m[467]#011val's binary_logloss: 0.166555#011train's binary_logloss: 0.0445084\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[468]#011val's binary_logloss: 0.1666#011train's binary_logloss: 0.0444343\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[469]#011val's binary_logloss: 0.196315#011train's binary_logloss: 0.0413097\u001b[0m\n",
      "\u001b[34m[470]#011val's binary_logloss: 0.196259#011train's binary_logloss: 0.0412424\u001b[0m\n",
      "\u001b[34m[471]#011val's binary_logloss: 0.196307#011train's binary_logloss: 0.0411453\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[469]#011val's binary_logloss: 0.166658#011train's binary_logloss: 0.0443478\u001b[0m\n",
      "\u001b[35m[470]#011val's binary_logloss: 0.16669#011train's binary_logloss: 0.04427\u001b[0m\n",
      "\u001b[35m[471]#011val's binary_logloss: 0.166731#011train's binary_logloss: 0.0441661\u001b[0m\n",
      "\u001b[34m[472]#011val's binary_logloss: 0.196381#011train's binary_logloss: 0.0410765\u001b[0m\n",
      "\u001b[34m[473]#011val's binary_logloss: 0.196461#011train's binary_logloss: 0.0409731\u001b[0m\n",
      "\u001b[35m[472]#011val's binary_logloss: 0.166786#011train's binary_logloss: 0.0440465\u001b[0m\n",
      "\u001b[35m[473]#011val's binary_logloss: 0.166811#011train's binary_logloss: 0.043913\u001b[0m\n",
      "\u001b[35m[474]#011val's binary_logloss: 0.166823#011train's binary_logloss: 0.0438278\u001b[0m\n",
      "\u001b[34m[474]#011val's binary_logloss: 0.196472#011train's binary_logloss: 0.040891\u001b[0m\n",
      "\u001b[34m[475]#011val's binary_logloss: 0.196535#011train's binary_logloss: 0.040843\u001b[0m\n",
      "\u001b[34m[476]#011val's binary_logloss: 0.19655#011train's binary_logloss: 0.0407512\u001b[0m\n",
      "\u001b[35m[475]#011val's binary_logloss: 0.1668#011train's binary_logloss: 0.0437472\u001b[0m\n",
      "\u001b[35m[476]#011val's binary_logloss: 0.166854#011train's binary_logloss: 0.0436672\u001b[0m\n",
      "\u001b[35m[477]#011val's binary_logloss: 0.166862#011train's binary_logloss: 0.0435711\u001b[0m\n",
      "\u001b[34m[477]#011val's binary_logloss: 0.196621#011train's binary_logloss: 0.0406539\u001b[0m\n",
      "\u001b[34m[478]#011val's binary_logloss: 0.196646#011train's binary_logloss: 0.0405783\u001b[0m\n",
      "\u001b[34m[479]#011val's binary_logloss: 0.196628#011train's binary_logloss: 0.0405048\u001b[0m\n",
      "\u001b[35m[478]#011val's binary_logloss: 0.166929#011train's binary_logloss: 0.0434935\u001b[0m\n",
      "\u001b[35m[479]#011val's binary_logloss: 0.166941#011train's binary_logloss: 0.0434246\u001b[0m\n",
      "\u001b[35m[480]#011val's binary_logloss: 0.166992#011train's binary_logloss: 0.0433584\u001b[0m\n",
      "\u001b[34m[480]#011val's binary_logloss: 0.196672#011train's binary_logloss: 0.0404406\u001b[0m\n",
      "\u001b[34m[481]#011val's binary_logloss: 0.196745#011train's binary_logloss: 0.0403234\u001b[0m\n",
      "\u001b[34m[482]#011val's binary_logloss: 0.19682#011train's binary_logloss: 0.0402266\u001b[0m\n",
      "\u001b[35m[481]#011val's binary_logloss: 0.167008#011train's binary_logloss: 0.0432575\u001b[0m\n",
      "\u001b[35m[482]#011val's binary_logloss: 0.167043#011train's binary_logloss: 0.04315\u001b[0m\n",
      "\u001b[35m[483]#011val's binary_logloss: 0.167085#011train's binary_logloss: 0.0430565\u001b[0m\n",
      "\u001b[34m[483]#011val's binary_logloss: 0.19685#011train's binary_logloss: 0.040149\u001b[0m\n",
      "\u001b[34m[484]#011val's binary_logloss: 0.196886#011train's binary_logloss: 0.0400452\u001b[0m\n",
      "\u001b[34m[485]#011val's binary_logloss: 0.197001#011train's binary_logloss: 0.0399528\u001b[0m\n",
      "\u001b[35m[484]#011val's binary_logloss: 0.167109#011train's binary_logloss: 0.0429576\u001b[0m\n",
      "\u001b[35m[485]#011val's binary_logloss: 0.167171#011train's binary_logloss: 0.0428516\u001b[0m\n",
      "\u001b[35m[486]#011val's binary_logloss: 0.167179#011train's binary_logloss: 0.0427452\u001b[0m\n",
      "\u001b[34m[486]#011val's binary_logloss: 0.19701#011train's binary_logloss: 0.039857\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[487]#011val's binary_logloss: 0.19697#011train's binary_logloss: 0.0397759\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[488]#011val's binary_logloss: 0.196961#011train's binary_logloss: 0.039701\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[487]#011val's binary_logloss: 0.167208#011train's binary_logloss: 0.0426707\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[488]#011val's binary_logloss: 0.167219#011train's binary_logloss: 0.0425934\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[489]#011val's binary_logloss: 0.167252#011train's binary_logloss: 0.0425253\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[489]#011val's binary_logloss: 0.1969#011train's binary_logloss: 0.0396196\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[490]#011val's binary_logloss: 0.196934#011train's binary_logloss: 0.0395484\u001b[0m\n",
      "\u001b[34m[491]#011val's binary_logloss: 0.196962#011train's binary_logloss: 0.0394638\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[490]#011val's binary_logloss: 0.16729#011train's binary_logloss: 0.0424418\u001b[0m\n",
      "\u001b[35m[491]#011val's binary_logloss: 0.167344#011train's binary_logloss: 0.0423161\u001b[0m\n",
      "\u001b[35m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[35m[492]#011val's binary_logloss: 0.167342#011train's binary_logloss: 0.042247\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\u001b[0m\n",
      "\u001b[34m[492]#011val's binary_logloss: 0.196999#011train's binary_logloss: 0.0394196\u001b[0m\n",
      "\u001b[34m[493]#011val's binary_logloss: 0.197012#011train's binary_logloss: 0.0393635\u001b[0m\n",
      "\u001b[34m[494]#011val's binary_logloss: 0.197044#011train's binary_logloss: 0.0393181\u001b[0m\n",
      "\u001b[35m[493]#011val's binary_logloss: 0.167354#011train's binary_logloss: 0.0421699\u001b[0m\n",
      "\u001b[35m[494]#011val's binary_logloss: 0.16735#011train's binary_logloss: 0.0420943\u001b[0m\n",
      "\u001b[35m[495]#011val's binary_logloss: 0.167387#011train's binary_logloss: 0.0419964\u001b[0m\n",
      "\u001b[34m[495]#011val's binary_logloss: 0.197048#011train's binary_logloss: 0.0392554\u001b[0m\n",
      "\u001b[34m[496]#011val's binary_logloss: 0.197061#011train's binary_logloss: 0.0391785\u001b[0m\n",
      "\u001b[34m[497]#011val's binary_logloss: 0.197073#011train's binary_logloss: 0.03911\u001b[0m\n",
      "\u001b[35m[496]#011val's binary_logloss: 0.167425#011train's binary_logloss: 0.0419188\u001b[0m\n",
      "\u001b[35m[497]#011val's binary_logloss: 0.167416#011train's binary_logloss: 0.0418428\u001b[0m\n",
      "\u001b[35m[498]#011val's binary_logloss: 0.167447#011train's binary_logloss: 0.0417444\u001b[0m\n",
      "\u001b[34m[498]#011val's binary_logloss: 0.197079#011train's binary_logloss: 0.0390127\u001b[0m\n",
      "\u001b[35m[499]#011val's binary_logloss: 0.167483#011train's binary_logloss: 0.0416575\u001b[0m\n",
      "\u001b[34m[499]#011val's binary_logloss: 0.197036#011train's binary_logloss: 0.0389274\u001b[0m\n",
      "\u001b[34m[500]#011val's binary_logloss: 0.197174#011train's binary_logloss: 0.0388396\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Finished linking network in 153.143809 seconds\u001b[0m\n",
      "\u001b[35m[500]#011val's binary_logloss: 0.167537#011train's binary_logloss: 0.0415617\u001b[0m\n",
      "\u001b[35m[LightGBM] [Info]\u001b[0m\n",
      "\u001b[35mFinished linking network in 147.974846 seconds\u001b[0m\n",
      "\u001b[35m2023-11-13 14:04:03,942 - distributed.worker - INFO - Stopping worker at tcp://10.0.219.142:9000. Reason: scheduler-remove-worker\u001b[0m\n",
      "\u001b[35m2023-11-13 14:04:03,945 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.0.219.142:41569'. Reason: scheduler-remove-worker\u001b[0m\n",
      "\u001b[35m2023-11-13 14:04:03,946 - distributed.core - INFO - Connection to tcp://10.0.228.115:8786 has been closed.\u001b[0m\n",
      "\u001b[35m2023-11-13 14:04:03,948 - distributed.nanny - INFO - Worker closed\u001b[0m\n",
      "\u001b[34mINFO:root:Done training\u001b[0m\n",
      "\u001b[34mINFO:root:Saving model...\u001b[0m\n",
      "\u001b[34mINFO:root:Info file not found at '_input_model_extracted/__models_info__.json'.\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,935 - distributed.scheduler - INFO - Retiring worker tcp://10.0.228.115:9000\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,935 - distributed.scheduler - INFO - Retiring worker tcp://10.0.219.142:9000\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,936 - distributed.active_memory_manager - INFO - Retiring worker tcp://10.0.228.115:9000; no unique keys need to be moved away.\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,936 - distributed.active_memory_manager - INFO - Retiring worker tcp://10.0.219.142:9000; no unique keys need to be moved away.\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,936 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.0.228.115:9000', status: closing_gracefully, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,936 - distributed.core - INFO - Removing comms to tcp://10.0.228.115:9000\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,936 - distributed.scheduler - INFO - Retired worker tcp://10.0.228.115:9000\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,936 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://10.0.219.142:9000', status: closing_gracefully, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,936 - distributed.core - INFO - Removing comms to tcp://10.0.219.142:9000\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,936 - distributed.scheduler - INFO - Lost all workers\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,936 - distributed.scheduler - INFO - Retired worker tcp://10.0.219.142:9000\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,942 - distributed.worker - INFO - Stopping worker at tcp://10.0.228.115:9000. Reason: scheduler-remove-worker\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,944 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.0.228.115:37323'. Reason: scheduler-remove-worker\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,945 - distributed.core - INFO - Received 'close-stream' from tcp://10.0.228.115:39886; closing.\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,946 - distributed.core - INFO - Connection to tcp://10.0.228.115:8786 has been closed.\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,946 - distributed.core - INFO - Received 'close-stream' from tcp://10.0.219.142:58070; closing.\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:03,948 - distributed.nanny - INFO - Worker closed\u001b[0m\n",
      "\u001b[35m2023-11-13 14:04:05,949 - distributed.nanny - ERROR - Worker process died unexpectedly\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:05,949 - distributed.nanny - ERROR - Worker process died unexpectedly\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:06,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.0.228.115:37323'. Reason: nanny-close-gracefully\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:06,153 - distributed.dask_worker - INFO - End worker\u001b[0m\n",
      "\u001b[35m2023-11-13 14:04:06,213 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.0.219.142:41569'. Reason: nanny-close-gracefully\u001b[0m\n",
      "\u001b[35m2023-11-13 14:04:06,213 - distributed.dask_worker - INFO - End worker\u001b[0m\n",
      "\u001b[35m2023-11-13 14:04:06,286 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-11-13 14:04:31 Uploading - Uploading generated training model\u001b[34m2023-11-13 14:04:23,956 - distributed.scheduler - INFO - Scheduler closing...\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:23,956 - distributed.scheduler - INFO - Scheduler closing all comms\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:23,957 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.0.228.115:8786'\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:23,958 - distributed.scheduler - INFO - End scheduler\u001b[0m\n",
      "\u001b[34m2023-11-13 14:04:24,141 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-11-13 14:04:42 Completed - Training job completed\n",
      "Training seconds: 606\n",
      "Billable seconds: 606\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "training_job_name = name_from_base(f\"built-in-algo-{train_model_id}-training\")\n",
    "\n",
    "# spot training hp define\n",
    "use_spot_instances = True\n",
    "max_run = 3600\n",
    "max_wait = 7200 if use_spot_instances else None\n",
    "checkpoint_s3_uri = (\n",
    "    \"s3://{}/{}/checkpoints/{}\".format(bucket, prefix, job_name) if use_spot_instances else None\n",
    ")\n",
    "print(\"Checkpoint path:\", checkpoint_s3_uri)\n",
    "\n",
    "\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tabular_estimator = Estimator(\n",
    "    role=role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=2, # for distributed training, specify an instance_count greater than 1\n",
    "    instance_type=training_instance_type,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    #use_spot_instances=use_spot_instances,\n",
    "    #max_run=max_run,\n",
    "    #max_wait=max_wait,\n",
    "    #checkpoint_s3_uri=checkpoint_s3_uri,\n",
    ")\n",
    "\n",
    "\n",
    "# Launch a SageMaker Training job by passing the S3 path of the training data\n",
    "tabular_estimator.fit(\n",
    "    {\n",
    "        \"train\": s3_input_train,\n",
    "        #\"validation\": test_data_uri,\n",
    "    }, logs=True, job_name=training_job_name#, wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3653b74c-949c-4631-a81f-828cb62b144b",
   "metadata": {},
   "source": [
    "2. distributed training with gpu\n",
    "- XGB：\n",
    "    - SageMaker XGBoost version 1.2-2 or later supports GPU training. Despite higher per-instance costs, GPUs train more quickly, making them more cost effective.\n",
    "    - SageMaker XGBoost version 1.2-2 or later supports P2, P3, G4dn, and G5 GPU instance families.\n",
    "    - SageMaker XGBoost version 1.7-1 or later supports P3, G4dn, and G5 GPU instance families. Note that due to compute capacity requirements, version 1.7-1 or later does not support the P2 instance family.\n",
    "    - To take advantage of GPU training, specify the instance type as one of the GPU instances (for example, P3) and set the 'tree_method' hyperparameter to gpu_hist in your existing XGBoost script.\n",
    "    \n",
    "- Distributed training with single-GPU instances\n",
    "\n",
    "    - SageMaker XGBoost versions 1.2-2 through 1.3-1 only support single-GPU instance training. This means that even if you select a multi-GPU instance, only one GPU is used per instance.\n",
    "\n",
    "    - If you use XGBoost versions 1.2-2 through 1.3-1, or if you do not need to use multi-GPU instances, then you must divide your input data between the total number of instances. For more information, see Divide input data across instances.\n",
    "\n",
    "    - Note\n",
    "        - Versions 1.2-2 through 1.3-1 of SageMaker XGBoost only use one GPU per instance even if you choose a multi-GPU instance.\n",
    "\n",
    "- Distributed training with multi-GPU instances\n",
    "    - Starting with version 1.5-1, SageMaker XGBoost offers distributed GPU training with Dask. With Dask you can utilize all GPUs when using one or more multi-GPU instances. Dask also works when using single-GPU instances.\n",
    "\n",
    "    - Train with Dask using the following steps:\n",
    "\n",
    "        - Either omit the 'distribution' parameter in your TrainingInput or set it to FullyReplicated.\n",
    "        - When defining your hyperparameters, set 'use_dask_gpu_training' to \"true\".\n",
    "- Catboost:\n",
    "    - byoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2fc67fb5-f307-48f3-bdab-7a4203d261ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "# sample code for converting parquet data\n",
    "'''\n",
    "import pyarrow\n",
    "%%time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "# Split the downloaded data into train/test dataframes\n",
    "train, test = np.split(data.sample(frac=1), [int(0.8 * len(data))])\n",
    "# requires PyArrow installed\n",
    "train.to_parquet(\"77_train.parquet\")\n",
    "test.to_parquet(\"77_test.parquet\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a48505-164d-41bc-a58c-f525a9461410",
   "metadata": {
    "tags": []
   },
   "source": [
    "### define data source and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826666d0-019a-44b0-b369-2eaec4b19790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefix = \"77_dis\"\n",
    "training_data_uri = 's3://{}/{}/train/7k7k_train.parquet'.format(bucket,prefix)\n",
    "test_data_uri = 's3://{}/{}/validation/7k7k_test.parquet'.format(bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b23da325-bec6-4612-bf35-56aa4bc16c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.7-1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "container = sagemaker.image_uris.retrieve(\"xgboost\", sess.boto_region_name, \"1.7-1\")\n",
    "display(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe65a3-a021-4d71-a873-2ce6361d5697",
   "metadata": {},
   "source": [
    "### 用sagemaker 的Traininput 定义训练数据channel，指定数据分发方式，格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b6a33877-297e-4462-850c-d9117aa76e49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "s3_input_train = TrainingInput(\n",
    "    training_data_uri, distribution= \"FullyReplicated\", content_type=\"application/x-parquet\"\n",
    ")\n",
    "s3_input_validation = TrainingInput(\n",
    "    test_data_uri, distribution= \"FullyReplicated\", content_type=\"application/x-parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "98c400a7-d0ee-4ed7-8095-32f32477632e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-11-13-14-52-46-297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-13 14:52:46 Starting - Starting the training job...\n",
      "2023-11-13 14:53:12 Starting - Preparing the instances for training.........\n",
      "2023-11-13 14:54:32 Downloading - Downloading input data...\n",
      "2023-11-13 14:55:02 Training - Downloading the training image...\n",
      "2023-11-13 14:55:33 Training - Training image download completed. Training in progress...\u001b[34m[2023-11-13 14:55:57.214 ip-10-2-113-198.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-11-13 14:55:57.243 ip-10-2-113-198.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:55:57:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:55:57:INFO] Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:55:57:INFO] Failed to parse hyperparameter tree_method value gpu_hist to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:55:57:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:55:57:INFO] Determined 1 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:55:57:INFO] Going to run distributed GPU training through Dask.\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:03,804 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:03,805 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:03,828 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m[2023-11-13 14:55:59.561 ip-10-2-119-184.ec2.internal:8 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-11-13 14:55:59.592 ip-10-2-119-184.ec2.internal:8 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:56:00:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:56:00:INFO] Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:56:00:INFO] Failed to parse hyperparameter tree_method value gpu_hist to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:56:00:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:56:00:INFO] Determined 1 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:56:00:INFO] Going to run distributed GPU training through Dask.\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/distributed/cli/dask_scheduler.py:140: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:00,703 - distributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:00,707 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:00,710 - distributed.scheduler - INFO - State start\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:00,730 - distributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:00,731 - distributed.scheduler - INFO -   Scheduler at:   tcp://10.2.119.184:8786\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:00,731 - distributed.scheduler - INFO -   dashboard at:                     :8787\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:00,829 - distributed.scheduler - INFO - Receive client connection: Client-c2154cce-8234-11ee-8008-c2fa11f3de45\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:01,067 - distributed.core - INFO - Starting established connection to tcp://10.2.119.184:45540\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:01,080 - distributed.scheduler - INFO - Remove client Client-c2154cce-8234-11ee-8008-c2fa11f3de45\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:01,080 - distributed.core - INFO - Received 'close-stream' from tcp://10.2.119.184:45540; closing.\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:01,080 - distributed.scheduler - INFO - Remove client Client-c2154cce-8234-11ee-8008-c2fa11f3de45\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:01,081 - distributed.scheduler - INFO - Close client connection: Client-c2154cce-8234-11ee-8008-c2fa11f3de45\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:01,088 - distributed.scheduler - INFO - Receive client connection: Client-c2a8ed05-8234-11ee-8008-c2fa11f3de45\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:01,089 - distributed.core - INFO - Starting established connection to tcp://10.2.119.184:45568\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:01,328 - distributed.scheduler - INFO - Receive client connection: Client-c0aa8f82-8234-11ee-8007-b6d503c79352\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:01,329 - distributed.core - INFO - Starting established connection to tcp://10.2.113.198:45842\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:01,342 - distributed.scheduler - INFO - Remove client Client-c0aa8f82-8234-11ee-8007-b6d503c79352\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:01,342 - distributed.core - INFO - Received 'close-stream' from tcp://10.2.113.198:45842; closing.\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:01,343 - distributed.scheduler - INFO - Remove client Client-c0aa8f82-8234-11ee-8007-b6d503c79352\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:01,343 - distributed.scheduler - INFO - Close client connection: Client-c0aa8f82-8234-11ee-8007-b6d503c79352\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:03,537 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:03,537 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:03,560 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,323 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.2.113.198:33135', status: init, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,324 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.2.113.198:33135\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,324 - distributed.core - INFO - Starting established connection to tcp://10.2.113.198:45866\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,499 - distributed.worker - INFO -       Start worker at:   tcp://10.2.119.184:45557\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,499 - distributed.worker - INFO -          Listening to:   tcp://10.2.119.184:45557\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,499 - distributed.worker - INFO -          dashboard at:         10.2.119.184:44739\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,499 - distributed.worker - INFO - Waiting to connect to:    tcp://10.2.119.184:8786\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,499 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,499 - distributed.worker - INFO -               Threads:                          1\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,500 - distributed.worker - INFO -                Memory:                  57.12 GiB\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,500 - distributed.worker - INFO -       Local Directory: /home/model-server/tmp/dask-worker-space/worker-m1651swa\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,500 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed9a1a7e-6834-4ef1-b53e-4381f3fc89f3\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,500 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3309791-6ad6-479d-85c0-448cdae47157\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,500 - distributed.worker - INFO - Starting Worker plugin PreImport-255d09b4-3145-4bd4-b8ff-03abc9fc8082\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,500 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,506 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://10.2.119.184:45557', status: init, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,507 - distributed.scheduler - INFO - Starting worker compute stream, tcp://10.2.119.184:45557\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,507 - distributed.core - INFO - Starting established connection to tcp://10.2.119.184:45600\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,507 - distributed.worker - INFO -         Registered to:    tcp://10.2.119.184:8786\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,507 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:04,508 - distributed.core - INFO - Starting established connection to tcp://10.2.119.184:8786\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:56:04:INFO] Starting to read training data...\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,316 - distributed.worker - INFO -       Start worker at:   tcp://10.2.113.198:33135\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,316 - distributed.worker - INFO -          Listening to:   tcp://10.2.113.198:33135\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,317 - distributed.worker - INFO -          dashboard at:         10.2.113.198:43919\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,317 - distributed.worker - INFO - Waiting to connect to:    tcp://10.2.119.184:8786\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,317 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,317 - distributed.worker - INFO -               Threads:                          1\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,317 - distributed.worker - INFO -                Memory:                  57.32 GiB\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,317 - distributed.worker - INFO -       Local Directory: /home/model-server/tmp/dask-worker-space/worker-hknj9knz\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,317 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2c30c07a-e79e-4ce0-9511-607e3f2f6d90\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,317 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0b9b359e-e8a6-479f-9f0b-6d9b8e69a65f\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,317 - distributed.worker - INFO - Starting Worker plugin PreImport-f8f98920-6f01-4985-928e-b26490451b83\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,318 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,324 - distributed.worker - INFO -         Registered to:    tcp://10.2.119.184:8786\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,324 - distributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:04,326 - distributed.core - INFO - Starting established connection to tcp://10.2.119.184:8786\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:56:07:INFO] Train features matrix has 15191 rows and 1192 columns\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:56:08:INFO] Data load complete. Starting training...\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:09,110 - distributed.worker - INFO - Run out-of-band function '_start_tracker'\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:09,461 - distributed.scheduler - INFO - Receive client connection: Client-worker-c7a68ecb-8234-11ee-802c-b6d503c79352\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:09,462 - distributed.core - INFO - Starting established connection to tcp://10.2.113.198:47246\u001b[0m\n",
      "\u001b[35m[14:56:09] task [xgboost.dask-tcp://10.2.113.198:33135]:tcp://10.2.113.198:33135 got new rank 0\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:0.41823#011validation-rmse:0.41901\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:0.41823#011validation-rmse:0.41901\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:0.35608#011validation-rmse:0.35776\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:0.35608#011validation-rmse:0.35776\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:0.31032#011validation-rmse:0.31285\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:0.31032#011validation-rmse:0.31285\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:0.27700#011validation-rmse:0.28039\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:0.27700#011validation-rmse:0.28039\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:0.25368#011validation-rmse:0.25784\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:0.25368#011validation-rmse:0.25784\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:0.23732#011validation-rmse:0.24217\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:0.23732#011validation-rmse:0.24217\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:0.22613#011validation-rmse:0.23156\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:0.22613#011validation-rmse:0.23156\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:0.21869#011validation-rmse:0.22458\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:0.21869#011validation-rmse:0.22458\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:0.21383#011validation-rmse:0.22008\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:0.21383#011validation-rmse:0.22008\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:0.21067#011validation-rmse:0.21719\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:0.21067#011validation-rmse:0.21719\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:0.20860#011validation-rmse:0.21534\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:0.20860#011validation-rmse:0.21534\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:0.20730#011validation-rmse:0.21420\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:0.20730#011validation-rmse:0.21420\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:0.20647#011validation-rmse:0.21350\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:0.20647#011validation-rmse:0.21350\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:0.20588#011validation-rmse:0.21301\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:0.20588#011validation-rmse:0.21301\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:0.20553#011validation-rmse:0.21273\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:0.20553#011validation-rmse:0.21273\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:0.20530#011validation-rmse:0.21256\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:0.20530#011validation-rmse:0.21256\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:0.20517#011validation-rmse:0.21247\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:0.20517#011validation-rmse:0.21247\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:0.20508#011validation-rmse:0.21242\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:0.20508#011validation-rmse:0.21242\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:0.20503#011validation-rmse:0.21239\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:0.20503#011validation-rmse:0.21239\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:0.20500#011validation-rmse:0.21237\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:0.20500#011validation-rmse:0.21237\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:0.20497#011validation-rmse:0.21237\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:0.20497#011validation-rmse:0.21237\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:0.20495#011validation-rmse:0.21236\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:0.20495#011validation-rmse:0.21236\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:0.20494#011validation-rmse:0.21236\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:0.20494#011validation-rmse:0.21236\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:0.20494#011validation-rmse:0.21236\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:0.20494#011validation-rmse:0.21236\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:0.20494#011validation-rmse:0.21236\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:0.20494#011validation-rmse:0.21236\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:0.20493#011validation-rmse:0.21237\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:0.20493#011validation-rmse:0.21237\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:0.20493#011validation-rmse:0.21237\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:0.20493#011validation-rmse:0.21237\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:0.20493#011validation-rmse:0.21237\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:0.20493#011validation-rmse:0.21237\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:0.20492#011validation-rmse:0.21238\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:0.20492#011validation-rmse:0.21238\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:0.20492#011validation-rmse:0.21238\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:0.20492#011validation-rmse:0.21238\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:0.20492#011validation-rmse:0.21238\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:0.20492#011validation-rmse:0.21238\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:0.20492#011validation-rmse:0.21238\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:0.20492#011validation-rmse:0.21238\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[47]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[47]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[48]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[48]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[49]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[49]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[50]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[50]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[51]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[51]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[52]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[52]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[53]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[53]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[54]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[54]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[55]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[55]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[56]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[56]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[57]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[57]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[58]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[58]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[59]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[59]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[60]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[60]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[61]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[61]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[62]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[62]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[63]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[63]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[64]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[64]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[65]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[65]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[66]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[66]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[67]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[67]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[68]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[68]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[69]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[69]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[70]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[70]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[71]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[71]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[72]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[72]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[73]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[73]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[74]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[74]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[75]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[75]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[76]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[76]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[77]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[77]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[78]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[78]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[79]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[79]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[80]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[80]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[81]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[81]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[82]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[82]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[83]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[83]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[84]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[84]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[85]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[85]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[86]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[86]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[87]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[87]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[88]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[88]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[89]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[89]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[90]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[90]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[91]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[91]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[92]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[92]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[93]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[93]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[94]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[94]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[95]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[95]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[96]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[96]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[97]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[97]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[98]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[98]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[99]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[99]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[100]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[100]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[101]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[101]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[102]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[102]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[103]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[103]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[104]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[104]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[105]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[105]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[106]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[106]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[107]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[107]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[108]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[108]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[109]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[109]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[110]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[110]#011train-rmse:0.20438#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[111]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[111]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[112]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[112]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[113]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[113]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[114]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[114]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[115]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[115]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[116]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[116]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[117]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[117]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[118]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[118]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[119]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[119]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[120]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[120]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[121]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[121]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[122]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[122]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[123]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[123]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[124]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[124]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[125]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[125]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[126]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[126]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[127]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[127]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[128]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[128]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[129]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[129]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[130]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[130]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[131]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[131]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[132]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[132]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[133]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[133]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[134]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[134]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[135]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[135]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[136]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[136]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[137]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[137]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[138]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[138]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[139]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[139]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[140]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[140]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[141]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[141]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[142]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[142]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[143]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[143]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[144]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[144]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[145]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[145]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[146]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[146]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[147]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[147]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[148]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[148]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[149]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[149]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[150]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[150]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[151]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[151]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[152]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[152]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[153]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[153]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[154]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[154]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[155]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[155]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[156]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[156]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[157]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[157]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[158]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[158]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[159]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[159]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[160]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[160]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[161]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[161]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[162]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[162]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[163]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[163]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[164]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[164]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[165]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[165]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[166]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[166]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[167]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[167]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[168]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[168]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[169]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[169]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[170]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[170]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[171]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[171]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[172]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[172]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[173]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[173]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[174]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[174]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[175]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[175]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[176]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[176]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[177]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[177]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[178]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[178]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[179]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[179]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[180]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[180]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[181]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[181]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[182]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[182]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[183]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[183]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[184]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[184]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[185]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[185]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[186]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[186]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[187]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[187]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[188]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[188]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[189]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[189]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[190]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[190]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[191]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[191]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[192]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[192]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[193]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[193]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[194]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[194]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[195]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[195]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[196]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[196]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[197]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[197]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[198]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[198]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[199]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[199]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[200]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[200]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[201]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[201]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[202]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[202]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[203]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[203]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[204]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[204]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[205]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[205]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[206]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[206]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[207]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[207]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[208]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[208]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[209]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[209]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[210]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[210]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[211]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[211]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[212]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[212]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[213]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[213]#011train-rmse:0.20438#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[214]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[214]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[215]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[215]#011train-rmse:0.20438#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[216]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[216]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[217]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[217]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[218]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[218]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[219]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[219]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[220]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[220]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[221]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[221]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[222]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[222]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[223]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[223]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[224]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[224]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[225]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[225]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[226]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[226]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[227]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[227]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[228]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[228]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[229]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[229]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[230]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[230]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[231]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[231]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[232]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[232]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[233]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[233]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[234]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[234]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[235]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[235]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[236]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[236]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[237]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[237]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[238]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[238]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[239]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[239]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[240]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[240]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[241]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[241]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[242]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[242]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[243]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[243]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[244]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[244]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[245]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[245]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[246]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[246]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[247]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[247]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[248]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[248]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[249]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[249]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[250]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[250]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[251]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[251]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[252]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[252]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[253]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[253]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[254]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[254]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[255]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[255]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[256]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[256]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[257]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[257]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[258]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[258]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[259]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[259]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[260]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[260]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[261]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[261]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[262]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[262]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[263]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[263]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[264]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[264]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[265]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[265]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[266]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[266]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[267]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[267]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[268]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[268]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[269]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[269]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[270]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[270]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[271]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[271]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[272]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[272]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[273]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[273]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[274]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[274]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[275]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[275]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[276]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[276]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[277]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[277]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[278]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[278]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[279]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[279]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[280]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[280]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[281]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[281]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[282]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[282]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[283]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[283]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[284]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[284]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[285]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[285]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[286]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[286]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[287]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[287]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[288]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[288]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[289]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[289]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[290]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[290]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[291]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[291]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[292]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[292]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[293]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[293]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[294]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[294]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[295]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[295]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[296]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[296]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[297]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[297]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[298]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[298]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[299]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[299]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[300]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[300]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[301]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[301]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[302]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[302]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[303]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[303]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[304]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[304]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[305]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[305]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[306]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[306]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[307]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[307]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[308]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[308]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[309]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[309]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[310]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[310]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[311]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[311]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[312]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[312]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[313]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[313]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[314]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[314]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[315]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[315]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[316]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[316]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[317]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[317]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[318]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[318]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[319]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[319]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[320]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[320]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[321]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[321]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[322]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[322]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[323]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[323]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[324]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[324]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[325]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[325]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[326]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[326]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[327]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[327]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[328]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[328]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[329]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[329]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[330]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[330]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[331]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[331]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[332]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[332]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[333]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[333]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[334]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[334]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[335]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[335]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[336]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[336]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[337]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[337]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[338]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[338]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[339]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[339]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[340]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[340]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[341]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[341]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[342]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[342]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[343]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[343]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[344]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[344]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[345]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[345]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[346]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[346]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[347]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[347]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[348]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[348]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[349]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[349]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[350]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[350]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[351]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[351]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[352]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[352]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[353]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[353]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[354]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[354]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[355]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[355]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[356]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[356]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[357]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[357]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[358]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[358]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[359]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[359]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[360]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[360]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[361]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[361]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[362]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[362]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[363]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[363]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[364]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[364]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[365]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[365]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[366]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[366]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[367]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[367]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[368]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[368]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[369]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[369]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[370]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[370]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[371]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[371]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[372]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[372]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[373]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[373]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[374]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[374]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[375]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[375]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[376]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[376]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[377]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[377]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[378]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[378]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[379]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[379]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[380]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[380]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[381]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[381]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[382]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[382]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[383]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[383]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[384]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[384]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[385]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[385]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[386]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[386]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[387]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[387]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[388]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[388]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[389]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[389]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[390]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[390]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[391]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[391]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[392]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[392]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[393]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[393]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[394]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[394]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[395]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[395]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[396]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[396]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[397]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[397]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[398]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[398]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[399]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[399]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[400]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[400]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[401]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[401]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[402]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[402]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[403]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[403]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[404]#011train-rmse:0.20416#011validation-rmse:0.21203\u001b[0m\n",
      "\u001b[34m[404]#011train-rmse:0.20416#011validation-rmse:0.21203\u001b[0m\n",
      "\u001b[34m[405]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[405]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[406]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[406]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[407]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[407]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[408]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[408]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[409]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[409]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[410]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[410]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[411]#011train-rmse:0.20416#011validation-rmse:0.21203\u001b[0m\n",
      "\u001b[34m[411]#011train-rmse:0.20416#011validation-rmse:0.21203\u001b[0m\n",
      "\u001b[34m[412]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[412]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[413]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[413]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[414]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[414]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[415]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[415]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[416]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[416]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[417]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[417]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[418]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[418]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[419]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[419]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[420]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[420]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[421]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[421]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[422]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[422]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[423]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[423]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[424]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[424]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[425]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[425]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[426]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[426]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[427]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[427]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[428]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[428]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[429]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[429]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[430]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[430]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[431]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[431]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[432]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[432]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[433]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[433]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[434]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[434]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[435]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[435]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[436]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[436]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[437]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[437]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[438]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[438]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[439]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[439]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[440]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[440]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[441]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[441]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[442]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[442]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[443]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[443]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[444]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[444]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[445]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[445]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[446]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[446]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[447]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[447]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[448]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[448]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[449]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[449]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[450]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[450]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[451]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[451]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[452]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[452]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[453]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[453]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[454]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[454]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[455]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[455]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[456]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[456]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[457]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[457]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[458]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[458]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[459]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[459]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[460]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[460]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[461]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[461]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[462]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[462]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[463]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[463]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[464]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[464]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[465]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[465]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[466]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[466]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[467]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[467]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[468]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[468]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[469]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[469]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[470]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[470]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[471]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[471]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[472]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[472]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[473]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[473]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[474]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[474]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[475]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[475]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[476]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[476]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[477]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[477]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[478]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[478]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[479]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[479]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[480]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[480]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[481]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[481]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[482]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[482]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[483]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[483]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[484]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[484]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[485]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[485]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[486]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[486]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[487]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[487]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[488]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[488]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[489]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[489]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[490]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[490]#011train-rmse:0.20416#011validation-rmse:0.21202\u001b[0m\n",
      "\u001b[34m[491]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[491]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[492]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[492]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[493]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[493]#011train-rmse:0.20416#011validation-rmse:0.21201\u001b[0m\n",
      "\u001b[34m[494]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[494]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[495]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[495]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[496]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[496]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[497]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[497]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[498]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[498]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[499]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[499]#011train-rmse:0.20416#011validation-rmse:0.21200\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:56:18:INFO] Training complete. Saving model...\u001b[0m\n",
      "\u001b[34m[2023-11-13:14:56:19:INFO] Terminating cluster...\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:19,011 - distributed.scheduler - INFO - Remove client Client-c2a8ed05-8234-11ee-8008-c2fa11f3de45\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:19,011 - distributed.core - INFO - Received 'close-stream' from tcp://10.2.119.184:45568; closing.\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:19,012 - distributed.scheduler - INFO - Remove client Client-c2a8ed05-8234-11ee-8008-c2fa11f3de45\u001b[0m\n",
      "\u001b[34m2023-11-13 14:56:19,012 - distributed.scheduler - INFO - Close client connection: Client-c2a8ed05-8234-11ee-8008-c2fa11f3de45\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:19,075 - distributed.core - INFO - Connection to tcp://10.2.119.184:8786 has been closed.\u001b[0m\n",
      "\u001b[35m2023-11-13 14:56:19,075 - distributed.worker - INFO - Stopping worker at tcp://10.2.113.198:33135. Reason: worker-handle-scheduler-connection-broken\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/distributed/utils.py\", line 742, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/distributed/client.py\", line 1298, in _reconnect\n",
      "    await self._ensure_connected(timeout=timeout)\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/distributed/client.py\", line 1328, in _ensure_connected\n",
      "    comm = await connect(\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/distributed/comm/core.py\", line 315, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/miniconda3/lib/python3.8/asyncio/tasks.py\", line 659, in sleep\n",
      "    return await future\u001b[0m\n",
      "\u001b[35masyncio.exceptions.CancelledError\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/distributed/utils.py\", line 742, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/distributed/client.py\", line 1508, in _handle_report\n",
      "    await self._reconnect()\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/distributed/utils.py\", line 742, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/distributed/client.py\", line 1298, in _reconnect\n",
      "    await self._ensure_connected(timeout=timeout)\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/distributed/client.py\", line 1328, in _ensure_connected\n",
      "    comm = await connect(\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/distributed/comm/core.py\", line 315, in connect\n",
      "    await asyncio.sleep(backoff)\n",
      "  File \"/miniconda3/lib/python3.8/asyncio/tasks.py\", line 659, in sleep\n",
      "    return await future\u001b[0m\n",
      "\u001b[35masyncio.exceptions.CancelledError\u001b[0m\n",
      "\u001b[35m[2023-11-13:14:56:21:INFO] Received a shutdown signal from scheduler. Exiting...\u001b[0m\n",
      "\n",
      "2023-11-13 14:56:40 Uploading - Uploading generated training model\n",
      "2023-11-13 14:56:40 Completed - Training job completed\n",
      "Training seconds: 254\n",
      "Billable seconds: 254\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.p3.2xlarge\",\n",
    "    output_path=\"s3://{}/{}/output\".format(bucket, prefix),\n",
    "    sagemaker_session=sess,\n",
    "    # dependency = equirement.txt\n",
    ")\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    max_depth=6,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=7.5,\n",
    "    subsample=0.8,\n",
    "    verbosity=0,\n",
    "    objective=\"reg:squaredlogerror\",\n",
    "    num_round=500,\n",
    "    use_dask_gpu_training = \"true\",\n",
    "    tree_method = \"gpu_hist\"\n",
    ")\n",
    "# Used only if tree_method is set to gpu_hist.\n",
    "# deterministic_histogram:Default value: \"true\"\n",
    "# Used only if tree_method is set to hist or gpu_hist.\n",
    "# single_precision_histogram:Default value: \"false\"\n",
    "\n",
    "\n",
    "xgb.fit({\"train\": s3_input_train, \"validation\": s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c11517d3-ad34-496e-9713-e1ce9bfaf535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-xgboost-2023-11-13-14-52-46-297\n"
     ]
    }
   ],
   "source": [
    "print(xgb.latest_training_job.job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a020d-489b-4d6f-a78c-5a4fb5060a6b",
   "metadata": {},
   "source": [
    "### Model deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31cff0c8-0764-49c5-9672-07416454477d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json \n",
    "import sagemaker\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf28816-286e-43d2-9ce8-4fd806f27060",
   "metadata": {},
   "source": [
    "通过查找过去训练任务的方式恢复训练好的模型的所在位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a52050f4-1529-4177-8056-05449411bda6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "from datetime import date, datetime\n",
    "pp = pprint.PrettyPrinter(indent=4, depth=4)\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "response = sagemaker_client.list_training_jobs(SortBy='CreationTime',\n",
    "                                               NameContains='lightgbm',\n",
    "                                               CreationTimeAfter=datetime(2022, 12, 1))\n",
    "lgm_trainjob_name = response['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "\n",
    "response = sagemaker_client.list_training_jobs(SortBy='CreationTime',\n",
    "                                               NameContains='xgboost',\n",
    "                                               CreationTimeAfter=datetime(2022, 12, 1))\n",
    "xgb_trainjob_name = response['TrainingJobSummaries'][0]['TrainingJobName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bdb892-c646-4d20-b945-723ad8e92636",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 's3://sagemaker-us-east-1-2xxxxxxx60/example-tabular-training/output'\n",
    "lgm_model_uri = output_path +\"/\" + lgm_trainjob_name + '/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fc7043-d7ec-42d0-bf96-3b524cf8e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "xgb_model_uri = output_path +\"/\" + xgb_trainjob_name + '/output/model.tar.gz'\n",
    "xgb_model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe70e0b-1bff-4524-ac3a-dbbfbe9efef6",
   "metadata": {},
   "source": [
    "部署训练好的模型，部署的过程需要等待十几分钟的时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82255bcf-65c7-4548-adb3-aa58004aabe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", sess.boto_region_name, \"1.7-1\")\n",
    "\n",
    "# modify the uri with your model uri\n",
    "xgb = Model(image_uri=container, \n",
    "            model_data=xgb_model_uri, \n",
    "            role=role)\n",
    "                 \n",
    "xgb_predictor = xgb.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.xlarge\", serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c8b22-4f17-4f2d-8114-8d95b4dca2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the uri with your model uri\n",
    "lgm = Model(image_uri=deploy_image_uri, \n",
    "            model_data=lgm_model_uri, \n",
    "            role=role)\n",
    "lgm_predictor = lgm.deploy(\n",
    "    initial_instance_count=1, endpoint_name = endpoint_name, instance_type=inference_instance_type, serializer=CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c21058-7821-44ac-96fb-a70d0948e5f8",
   "metadata": {},
   "source": [
    "### 通过describe endpoint的接口查询推理节点部署情况\n",
    " - 文档：https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/describe_endpoint.html# \n",
    "\n",
    "返回的 “Status” 字段为部署的状态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47bd9d-cdf0-4044-9b77-29c3fb958eb5",
   "metadata": {},
   "source": [
    "### Batch Inference\n",
    "API相关文档： https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_transform_job.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8fab80e-a915-4231-b938-f96f28e73883",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "\n",
      "2023-11-13 14:56:40 Starting - Preparing the instances for training\n",
      "2023-11-13 14:56:40 Downloading - Downloading input data\n",
      "2023-11-13 14:56:40 Training - Training image download completed. Training in progress.\n",
      "2023-11-13 14:56:40 Uploading - Uploading generated training model\n",
      "2023-11-13 14:56:40 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "#### prediction with 200,000 trees\n",
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "sess = sage.Session()\n",
    "\n",
    "#best_job =tuner.best_training_job()\n",
    "#print(\"Best Job name:\" , best_job)\n",
    "\n",
    "attached_estimator = sage.estimator.Estimator.attach(xgb_trainjob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f426e-90fe-457b-9b44-03fc09012c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path ='s3://'+bucket+'/sagemaker/DEMO/data/test-predictions/'\n",
    "input_path  ='s3://'+bucket+'/sagemaker/DEMO/data/test/'\n",
    "\n",
    "transformer = attached_estimator.transformer(instance_count=1, \n",
    "                                             instance_type='ml.p2.xlarge', \n",
    "                                             assemble_with='Line', \n",
    "                                             accept='text/csv',\n",
    "                                             max_payload=1,\n",
    "                                             output_path=output_path,\n",
    "                                             env = {'SAGEMAKER_MODEL_SERVER_TIMEOUT' : '3600' })\n",
    "transformer.transform(input_path, \n",
    "                      content_type='text/csv',\n",
    "                      split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971bfbd6-d8a4-405d-bace-e191643706d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
